{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f34e68de-29d3-415b-ba8f-235533f55bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed_value = 42  # Choose any integer value for the seed\n",
    "\n",
    "random.seed(seed_value)          # Set seed for random module\n",
    "np.random.seed(seed_value)       # Set seed for NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8d9fa0",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7250ceb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch 2.1.0+cu121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x14a140c2a750>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import torch\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "print(\"Using torch\", torch.__version__)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8715bcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import MovieLens100K, IGMCDataset\n",
    "\n",
    "mov = MovieLens100K('/tmp/movielens')[0]\n",
    "#doub = IGMCDataset('/tmp/douban', 'Douban')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e12d5e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 2625 nodes\n",
      "dataset has 160000 edges\n",
      "dataset has 943 users\n",
      "dataset has 1682 items\n"
     ]
    }
   ],
   "source": [
    "num_nodes = mov.num_nodes\n",
    "print('dataset has {} nodes'.format(num_nodes))\n",
    "\n",
    "num_edges = mov.num_edges\n",
    "print('dataset has {} edges'.format(num_edges))\n",
    "\n",
    "num_users = mov['user'].x.shape[0]\n",
    "print('dataset has {} users'.format(num_users))\n",
    "\n",
    "num_movies = mov['movie'].x.shape[0]\n",
    "print('dataset has {} items'.format(num_movies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c00ec60",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6d9538",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_threshold = 3  #@param {type: \"integer\"}: Ratings equal to or greater than 3 are positive items.\n",
    "\n",
    "config_dict = {\n",
    "    \"num_samples_per_user\": 500,\n",
    "    \"num_users\": num_users,\n",
    "\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 350,\n",
    "    \"lr\": 0.001,\n",
    "    \"weight_decay\": 1,\n",
    "\n",
    "    \"embedding_size\": 64,\n",
    "    \"num_layers\": 5,\n",
    "    \"mf_rank\": 8,\n",
    "\n",
    "    \"minibatch_per_print\": 100,\n",
    "    \"epochs_per_print\": 1,\n",
    "\n",
    "    \"val_frac\": 0.2,\n",
    "    \"test_frac\": 0.1,\n",
    "\n",
    "    \"K\": 10,\n",
    "\n",
    "    \"model_name\": \"model_movielens100k_lightgcn-att.pth\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6a56cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_list_to_adjacency(edge_index, ratings, user_num, item_num):\n",
    "    # Create an empty adjacency matrix\n",
    "    adjacency = torch.zeros(user_num, item_num)\n",
    "    \n",
    "    # Fill the adjacency matrix using the edge list\n",
    "    for i in range(edge_index.size(1)):\n",
    "        start_node = edge_index[0, i].item()\n",
    "        end_node = edge_index[1, i].item()\n",
    "        #adjacency[start_node, end_node] = ratings[i]\n",
    "        \n",
    "        if ratings[i] > rating_threshold:\n",
    "            adjacency[start_node, end_node] = 1  # Assuming it's an unweighted graph\n",
    "    \n",
    "    return adjacency\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "global_edge_index = edge_list_to_adjacency(\n",
    "    mov[('user', 'rates', 'movie')]['edge_index'],\n",
    "    mov[('user', 'rates', 'movie')]['rating'],\n",
    "    num_users,\n",
    "    num_movies\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0527433",
   "metadata": {},
   "source": [
    "# Model Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5bc295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim, Tensor\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.typing import Adj\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LightGCNConv(MessagePassing):\n",
    "    r\"\"\"The neighbor aggregation operator from the `\"LightGCN: Simplifying and\n",
    "    Powering Graph Convolution Network for Recommendation\"\n",
    "    <https://arxiv.org/abs/2002.02126#>`_ paper\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Size of each input sample, or :obj:`-1` to derive\n",
    "            the size from the first input(s) to the forward method.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        num_users (int): Number of users for recommendation.\n",
    "        num_items (int): Number of items to recommend.\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int,\n",
    "                 num_users: int, num_items: int, **kwargs):\n",
    "        super(LightGCNConv, self).__init__(**kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        pass  # There are no layer parameters to learn.\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj) -> Tensor:\n",
    "        \"\"\"Performs neighborhood aggregation for user/item embeddings.\"\"\"\n",
    "        user_item = \\\n",
    "                torch.zeros(self.num_users, self.num_items, device=x.device)\n",
    "        user_item[edge_index[:, 0], edge_index[:, 1]] = 1 # global_edge_index[edge_index[:, 0], edge_index[:, 1]]\n",
    "\n",
    "        att = \\\n",
    "                torch.zeros(self.num_users, self.num_items, device=x.device)\n",
    "        att[edge_index[:, 0], edge_index[:, 1]] = global_edge_index[edge_index[:, 0], edge_index[:, 1]]\n",
    "\n",
    "        user_neighbor_counts = torch.sum((user_item > 0), axis=1)\n",
    "        item_neightbor_counts = torch.sum((user_item > 0), axis=0)\n",
    "\n",
    "        # Compute weight for aggregation: 1 / sqrt(N_u * N_i)\n",
    "        weights = (user_item > 0) / torch.sqrt(\n",
    "                user_neighbor_counts.repeat(self.num_items, 1).T \\\n",
    "                * item_neightbor_counts.repeat(self.num_users, 1))\n",
    "        #weights = att / torch.sqrt(\n",
    "        #        user_neighbor_counts.repeat(self.num_items, 1).T \\\n",
    "        #        * item_neightbor_counts.repeat(self.num_users, 1))\n",
    "        weights = torch.nan_to_num(weights, nan=0)\n",
    "        out = torch.concat((weights.T @ x[:self.num_users],\n",
    "                            weights @ x[self.num_users:]), 0)\n",
    "        return out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
    "                                   self.out_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54dfd56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 config: dict,\n",
    "                 device=None,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_users  = config[\"n_users\"]\n",
    "        self.num_items  = config[\"m_items\"]\n",
    "        self.embedding_size = config[\"embedding_size\"]\n",
    "        self.in_channels = self.embedding_size\n",
    "        self.out_channels = self.embedding_size\n",
    "        self.num_layers = config[\"num_layers\"]\n",
    "\n",
    "        # 0-th layer embedding.\n",
    "        self.embedding_user_item = torch.nn.Embedding(\n",
    "            num_embeddings=self.num_users + self.num_items,\n",
    "            embedding_dim=self.embedding_size)\n",
    "        self.alpha = None\n",
    "\n",
    "        # random normal init seems to be a better choice when lightGCN actually\n",
    "        # don't use any non-linear activation function\n",
    "        nn.init.normal_(self.embedding_user_item.weight, std=0.1)\n",
    "        print('use NORMAL distribution initilizer')\n",
    "\n",
    "        self.f = nn.Sigmoid()\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(LightGCNConv(\n",
    "                self.embedding_size, self.embedding_size,\n",
    "                num_users=self.num_users, num_items=self.num_items, **kwargs))\n",
    "\n",
    "        for _ in range(1, self.num_layers):\n",
    "            self.convs.append(\n",
    "                LightGCNConv(\n",
    "                        self.embedding_size, self.embedding_size, \n",
    "                        num_users=self.num_users, num_items=self.num_items,\n",
    "                        **kwargs))\n",
    "\n",
    "        self.device = None\n",
    "        if device is not None:\n",
    "            self.convs.to(device)\n",
    "            self.device = device\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj, *args, **kwargs) -> Tensor:\n",
    "        xs: List[Tensor] = []\n",
    "\n",
    "        edge_index = torch.nonzero(edge_index)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index, *args, **kwargs)\n",
    "            if self.device is not None:\n",
    "                x = x.to(self.device)\n",
    "            xs.append(x)\n",
    "        xs = torch.stack(xs)\n",
    "        \n",
    "        self.alpha = 1 / (1 + self.num_layers) * torch.ones(xs.shape)\n",
    "        if self.device is not None:\n",
    "            self.alpha = self.alpha.to(self.device)\n",
    "            xs = xs.to(self.device)\n",
    "        x = (xs * self.alpha).sum(dim=0)  # Sum along K layers.\n",
    "        return x\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
    "                f'{self.out_channels}, num_layers={self.num_layers})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df0a3d7",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b49659ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUsersRating(model, users, data):\n",
    "    \"\"\" Get the embedding of users\n",
    "    INPUT:\n",
    "        model: the LightGCN model you are training on\n",
    "        users: this is the user index (note: use 0-indexed and not user number,\n",
    "            which is 1-indexed)\n",
    "        data: the entire data, used to fetch all users and all items\n",
    "    \"\"\"\n",
    "    all_users_items = model(model.embedding_user_item.weight.clone(), global_edge_index)\n",
    "    #print(\"USERS: \", users)\n",
    "    #print(\"ALL USERS ITMES: \", all_users_items)\n",
    "    #print(\"ALL USERS SHAPE: \", all_users_items.shape)\n",
    "    all_users = all_users_items[:len(data[\"user\"].x)]\n",
    "    items_emb = all_users_items[len(data[\"user\"].x):]\n",
    "    users_emb = all_users[users.long()]\n",
    "    rating = model.f(torch.matmul(users_emb, items_emb.t()))\n",
    "    #print(\"RATING: \", rating)\n",
    "    return rating\n",
    "\n",
    "def getEmbedding(model, users, pos, neg, data, mask):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "        model: the LightGCN model you are training on\n",
    "        users: this is the user index (note: use 0-indexed and not user number,\n",
    "            which is 1-indexed)\n",
    "        pos: positive index corresponding to an item that the user like\n",
    "        neg: negative index corresponding to an item that the user doesn't like\n",
    "        data: the entire data, used to fetch all users and all items\n",
    "        mask: Masking matrix indicating edges present in the current\n",
    "            train / validation / test set.\n",
    "    \"\"\"\n",
    "    # assuming we always search for users and items by their indices (instead of\n",
    "    # user/item number)\n",
    "    all_users_items = model(model.embedding_user_item.weight.clone(),\n",
    "                            global_edge_index * mask)\n",
    "    # print(all_users_items)\n",
    "    all_users = all_users_items[:len(data[\"user\"].x)]\n",
    "    all_items = all_users_items[len(data[\"user\"].x):]\n",
    "    users_emb = all_users[users]\n",
    "    pos_emb = all_items[pos]\n",
    "    neg_emb = all_items[neg]\n",
    "    n_user = len(data[\"user\"].x)\n",
    "    users_emb_ego = model.embedding_user_item(users)\n",
    "    # offset the index to fetch embedding from user_item\n",
    "    pos_emb_ego = model.embedding_user_item(pos + n_user)\n",
    "    neg_emb_ego = model.embedding_user_item(neg + n_user)\n",
    "    return users_emb, pos_emb, neg_emb, users_emb_ego, pos_emb_ego, neg_emb_ego"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0e5c7",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f2f9971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpr_loss(model, users, pos, neg, data, mask):\n",
    "    \"\"\" \n",
    "    INPUT:\n",
    "        model: the LightGCN model you are training on\n",
    "        users: this is the user index (note: use 0-indexed and not user number,\n",
    "            which is 1-indexed)\n",
    "        pos: positive index corresponding to an item that the user like\n",
    "            (0-indexed, note to index items starting from 0)\n",
    "        neg: negative index corresponding to an item that the user doesn't like\n",
    "        data: the entire data, used to fetch all users and all items\n",
    "        mask: Masking matrix indicating edges present in the current\n",
    "            train / validation / test set.\n",
    "    OUTPUT:\n",
    "        loss, reg_loss\n",
    "    \"\"\"\n",
    "    # assuming we always sample the same number of positive and negative sample\n",
    "    # per user\n",
    "    assert len(users) == len(pos) and len(users) == len(neg)\n",
    "    (users_emb, pos_emb, neg_emb, \n",
    "    userEmb0,  posEmb0, negEmb0) = getEmbedding(model, users.long(), pos.long(),\n",
    "                                                neg.long(), data, mask)\n",
    "    reg_loss = (1/2)*(userEmb0.norm(2).pow(2) + \n",
    "                        posEmb0.norm(2).pow(2)  +\n",
    "                        negEmb0.norm(2).pow(2))/float(len(users))\n",
    "    pos_scores = torch.mul(users_emb, pos_emb)\n",
    "    pos_scores = torch.sum(pos_scores, dim=1)\n",
    "    neg_scores = torch.mul(users_emb, neg_emb)\n",
    "    neg_scores = torch.sum(neg_scores, dim=1)\n",
    "    \n",
    "    loss = torch.mean(torch.nn.functional.softplus(neg_scores - pos_scores))\n",
    "    \n",
    "    return loss, reg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d150b0e4",
   "metadata": {},
   "source": [
    "# Personalized Top K Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e47c6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def personalized_topk(pred, K, user_indices, edge_index):\n",
    "    \"\"\"Computes TopK precision and recall.\n",
    "\n",
    "    Args:\n",
    "        pred: Predicted similarities between user and item.\n",
    "        K: Number of items to rank.\n",
    "        user_indices: Indices of users for each prediction in `pred`.\n",
    "        edge_index: User and item connection matrix.\n",
    "\n",
    "    Returns:\n",
    "        Average Top K precision and recall for users in `user_indices`.\n",
    "    \"\"\"\n",
    "    per_user_preds = collections.defaultdict(list)\n",
    "    for index, user in enumerate(user_indices):\n",
    "        per_user_preds[user.item()].append(pred[index].item())\n",
    "    precisions = 0.0\n",
    "    recalls = 0.0\n",
    "    for user, preds in per_user_preds.items():\n",
    "        while len(preds) < K:\n",
    "            preds.append(random.choice(range(edge_index.shape[1])))\n",
    "        top_ratings, top_items = torch.topk(torch.tensor(preds), K)\n",
    "        correct_preds = edge_index[user, top_items].sum().item()\n",
    "        total_pos = edge_index[user].sum().item()\n",
    "        precisions += correct_preds / K\n",
    "        recalls += correct_preds / total_pos if total_pos != 0 else 0\n",
    "    num_users = len(user_indices.unique())\n",
    "    return precisions / num_users, recalls / num_users\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6d53f4",
   "metadata": {},
   "source": [
    "# Training, Validation, and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1852ec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sample_pos_neg(data, mask, num_samples_per_user):\n",
    "    \"\"\"Samples (user, positive item, negative item) tuples per user.\n",
    "\n",
    "    If a user does not have a postive (negative) item, we choose an item\n",
    "    with unknown liking (an item without raw rating data).\n",
    "\n",
    "    Args:\n",
    "        data: Dataset object containing edge_index and raw ratings matrix.\n",
    "        mask: Masking matrix indicating edges present in the current\n",
    "            train / validation / test set.\n",
    "        num_samples_per_user: Number of samples to generate for each user.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor object of (user, positive item, negative item) samples.\n",
    "    \"\"\"\n",
    "    print(\"=====Starting to sample=====\")\n",
    "    start = time.time()\n",
    "    samples = []\n",
    "    all_items = set(range(len(data[\"movie\"].x)))\n",
    "    #print(data[\"movie\"].x)\n",
    "    #print(data[\"user\"].x)\n",
    "    #print(\"DEBUG: \", len(data[\"movie\"].x), len(data[\"user\"].x))\n",
    "    for user_index, user in enumerate(data[\"user\"].x):\n",
    "        #print(\"HERE: \", user_index, user)\n",
    "        pos_items = set(\n",
    "            torch.nonzero(global_edge_index[user_index])[:, 0].tolist())\n",
    "        #print(\"POSITIVE: \", pos_items)\n",
    "        unknown_items = all_items.difference(\n",
    "                set(\n",
    "                    torch.nonzero(global_edge_index[user_index])[:, 0].tolist()))\n",
    "        neg_items = all_items.difference(\n",
    "            set(pos_items)).difference(set(unknown_items))\n",
    "        unmasked_items = set(torch.nonzero(mask[user_index])[:, 0].tolist())\n",
    "        if len(unknown_items.union(pos_items)) == 0 or \\\n",
    "                len(unknown_items.union(neg_items)) == 0:\n",
    "            continue\n",
    "        for _ in range(num_samples_per_user):\n",
    "            if len(pos_items.intersection(unmasked_items)) == 0:\n",
    "                pos_item_index = random.choice(\n",
    "                    list(unknown_items.intersection(unmasked_items)))\n",
    "            else:\n",
    "                pos_item_index = random.choice(\n",
    "                    list(pos_items.intersection(unmasked_items)))\n",
    "            if len(neg_items.intersection(unmasked_items)) == 0:\n",
    "                neg_item_index = random.choice(\n",
    "                    list(unknown_items.intersection(unmasked_items)))\n",
    "            else:\n",
    "                neg_item_index = random.choice(\n",
    "                    list(neg_items.intersection(unmasked_items)))\n",
    "            samples.append((user_index, pos_item_index, neg_item_index))\n",
    "    end = time.time()\n",
    "    print(f\"=====Sampling completed (took {end - start} seconds)=====\")\n",
    "    return torch.tensor(samples, dtype=torch.int32)\n",
    "\n",
    "def sample_pos_neg(data, train_mask, val_mask, test_mask, num_samples_per_user):\n",
    "    \"\"\"Samples (user, positive item, negative item) tuples per user.\n",
    "\n",
    "    If a user does not have a postive (negative) item, we choose an item\n",
    "    with unknown liking (an item without raw rating data).\n",
    "\n",
    "    Args:\n",
    "        data: Dataset object containing edge_index and raw ratings matrix.\n",
    "        train_mask: Masking matrix indicating edges present in train set.\n",
    "        val_mask: Masking matrix indicating edges present in validation set.\n",
    "        test_mask: Masking matrix indicating edges present in test set.\n",
    "        num_samples_per_user: Number of samples to generate for each user.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor object of (user, positive item, negative item) samples for\n",
    "        train, validation and test.\n",
    "    \"\"\"\n",
    "    train_samples = _sample_pos_neg(data, train_mask, num_samples_per_user)\n",
    "    val_samples = _sample_pos_neg(data, val_mask, num_samples_per_user)\n",
    "    test_samples = _sample_pos_neg(data, test_mask, num_samples_per_user)\n",
    "    return train_samples, val_samples, test_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7d0c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = mov['user'].x.shape[0]\n",
    "m_items = mov['movie'].x.shape[0]\n",
    "\n",
    "def train_val_test_split(num_user, num_item, val_frac, test_frac):\n",
    "    \"\"\"\n",
    "    Return two mask matrices (M, N) that represents edges present in the\n",
    "    train and validation set\n",
    "    \"\"\"\n",
    "    # get number of edges masked for training and validation\n",
    "    num_train_replaced = round((test_frac+val_frac)*num_user*num_item)\n",
    "    num_val_show = round(val_frac*num_user*num_item)\n",
    "\n",
    "    # edges masked during training\n",
    "    indices_user = np.random.randint(0, num_user, num_train_replaced)\n",
    "    indices_item = np.random.randint(0, num_item, num_train_replaced)\n",
    "\n",
    "    # sample part of edges from training stage to be unmasked during\n",
    "    # validation\n",
    "    indices_val_user = np.random.choice(indices_user, num_val_show)\n",
    "    indices_val_item = np.random.choice(indices_item, num_val_show)\n",
    "\n",
    "    train_mask = torch.ones(num_user, num_item)\n",
    "    train_mask[indices_user, indices_item] = 0\n",
    "\n",
    "    val_mask = train_mask.clone()\n",
    "    val_mask[indices_val_user, indices_val_item] = 1\n",
    "\n",
    "    test_mask = torch.ones_like(train_mask)\n",
    "\n",
    "    return train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "train_mask, val_mask, test_mask = train_val_test_split(\n",
    "    num_user = n_users,\n",
    "    num_item = m_items,\n",
    "    val_frac = config_dict[\"val_frac\"],\n",
    "    test_frac = config_dict[\"test_frac\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47769412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE:  cuda\n",
      "use NORMAL distribution initilizer\n",
      "=====Starting to sample=====\n",
      "=====Sampling completed (took 8.836215734481812 seconds)=====\n",
      "=====Starting to sample=====\n",
      "=====Sampling completed (took 10.419390201568604 seconds)=====\n",
      "=====Starting to sample=====\n",
      "=====Sampling completed (took 10.422235250473022 seconds)=====\n",
      "#Training samples: 188600 #Validation samples: 188600 #Test samples: 188600\n",
      "Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_config = {\n",
    "    \"n_users\": n_users,\n",
    "    \"m_items\": m_items,\n",
    "    \"embedding_size\": config_dict[\"embedding_size\"],\n",
    "    \"num_layers\": config_dict[\"num_layers\"],\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE: \", device)\n",
    "lightGCN = LightGCN(model_config, device=device)\n",
    "\n",
    "num_samples_per_user = config_dict[\"num_samples_per_user\"]\n",
    "epochs = config_dict[\"epochs\"]\n",
    "batch_size = config_dict[\"batch_size\"]\n",
    "lr = config_dict[\"lr\"]\n",
    "weight_decay = config_dict[\"weight_decay\"]\n",
    "\n",
    "lightGCN.to(device)\n",
    "\n",
    "samples_train, samples_val, samples_test = sample_pos_neg(\n",
    "    mov, train_mask, val_mask, test_mask, num_samples_per_user)\n",
    "\n",
    "samples_train=samples_train.to(device)\n",
    "samples_val=samples_val.to(device)\n",
    "samples_test=samples_test.to(device)\n",
    "train_mask=train_mask.to(device)\n",
    "val_mask=val_mask.to(device)\n",
    "test_mask=test_mask.to(device)\n",
    "mov = mov.to(device)\n",
    "\n",
    "print(f\"#Training samples: {len(samples_train)}\",\n",
    "      f\"#Validation samples: {len(samples_val)}\",\n",
    "      f\"#Test samples: {len(samples_test)}\")\n",
    "\n",
    "optimizer = optim.Adam(lightGCN.parameters(), lr=lr)\n",
    "print(\"Optimizer:\", optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5467c096-bda5-465a-818e-23264197e9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on the 0 epoch\n",
      "Training on epoch 0 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 1.67147, and regularization loss is 0.978323.\n",
      " Top K precision = 0.09574468085106379, recall = 0.021480876561358237.\n",
      "Training on epoch 0 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 1.250814, and regularization loss is 0.557667.\n",
      " Top K precision = 0.09687499999999998, recall = 0.01794701844155102.\n",
      "Training on epoch 0 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 1.042808, and regularization loss is 0.349661.\n",
      " Top K precision = 0.10109890109890107, recall = 0.01927600059823075.\n",
      "Training on epoch 0 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.897573, and regularization loss is 0.204425.\n",
      " Top K precision = 0.10212765957446807, recall = 0.019835787008199473.\n",
      "Training on epoch 0 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.821002, and regularization loss is 0.127855.\n",
      " Top K precision = 0.11122448979591835, recall = 0.026019070638319507.\n",
      "Training on epoch 0 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.776866, and regularization loss is 0.083719.\n",
      " Top K precision = 0.10408163265306122, recall = 0.02527770668857216.\n",
      "Training on epoch 0 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.746784, and regularization loss is 0.053637.\n",
      " Top K precision = 0.11111111111111104, recall = 0.02348848523715587.\n",
      "Training on epoch 0 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.726111, and regularization loss is 0.032964.\n",
      " Top K precision = 0.10531914893617016, recall = 0.02412376852913724.\n",
      "Training on epoch 0 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.716651, and regularization loss is 0.023504.\n",
      " Top K precision = 0.12268041237113395, recall = 0.030739849059054532.\n",
      "Training on epoch 0 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.707668, and regularization loss is 0.014521.\n",
      " Top K precision = 0.11720430107526876, recall = 0.029787705415645606.\n",
      "Training on epoch 0 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.70358, and regularization loss is 0.010433.\n",
      " Top K precision = 0.1212765957446808, recall = 0.027962033509898503.\n",
      "Training on epoch 0 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.699174, and regularization loss is 0.006027.\n",
      " Top K precision = 0.09999999999999998, recall = 0.02716897858967243.\n",
      "Training on epoch 0 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.697446, and regularization loss is 0.004298.\n",
      " Top K precision = 0.12105263157894731, recall = 0.03106152202888615.\n",
      "Training on epoch 0 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.696098, and regularization loss is 0.002951.\n",
      " Top K precision = 0.11263157894736836, recall = 0.023301047328592683.\n",
      "Training on epoch 0 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.694831, and regularization loss is 0.001683.\n",
      " Top K precision = 0.09148936170212761, recall = 0.02268030509204209.\n",
      "Training on epoch 0 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.694266, and regularization loss is 0.001119.\n",
      " Top K precision = 0.1326315789473683, recall = 0.029329740510479294.\n",
      "Training on epoch 0 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.694234, and regularization loss is 0.001087.\n",
      " Top K precision = 0.08631578947368418, recall = 0.021722070097214677.\n",
      "Training on epoch 0 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693602, and regularization loss is 0.000455.\n",
      " Top K precision = 0.10312499999999998, recall = 0.020452600365594195.\n",
      "Training on epoch 0 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693555, and regularization loss is 0.000408.\n",
      " Top K precision = 0.10714285714285705, recall = 0.025850667255236177.\n",
      "\n",
      "Training on 0 epoch completed.\n",
      " Average bpr_loss on train set is 0.007957 for the current epoch.\n",
      " Training top K precision = 0.09098621420996811, recall = 0.0285732467620746.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.0873806998939553, recall = 0.02721027090123204.\n",
      "\n",
      "Training on the 1 epoch\n",
      "Training on epoch 1 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693474, and regularization loss is 0.000326.\n",
      " Top K precision = 0.12282608695652167, recall = 0.025892508811425397.\n",
      "Training on epoch 1 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693296, and regularization loss is 0.000149.\n",
      " Top K precision = 0.10531914893617018, recall = 0.034003982734852146.\n",
      "Training on epoch 1 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693272, and regularization loss is 0.000125.\n",
      " Top K precision = 0.12783505154639166, recall = 0.02911594781012561.\n",
      "Training on epoch 1 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693202, and regularization loss is 5.5e-05.\n",
      " Top K precision = 0.09292929292929292, recall = 0.026102952306669022.\n",
      "Training on epoch 1 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693203, and regularization loss is 5.6e-05.\n",
      " Top K precision = 0.10531914893617014, recall = 0.028755105879035445.\n",
      "Training on epoch 1 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693204, and regularization loss is 5.6e-05.\n",
      " Top K precision = 0.127659574468085, recall = 0.031323097738316694.\n",
      "Training on epoch 1 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693178, and regularization loss is 3.1e-05.\n",
      " Top K precision = 0.09263157894736837, recall = 0.029513680333137567.\n",
      "Training on epoch 1 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693186, and regularization loss is 3.9e-05.\n",
      " Top K precision = 0.11521739130434779, recall = 0.025919463721234236.\n",
      "Training on epoch 1 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693184, and regularization loss is 3.7e-05.\n",
      " Top K precision = 0.11958762886597936, recall = 0.023829718020296042.\n",
      "Training on epoch 1 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.69318, and regularization loss is 3.3e-05.\n",
      " Top K precision = 0.09130434782608692, recall = 0.02062011490541924.\n",
      "Training on epoch 1 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
      " Top K precision = 0.1263157894736841, recall = 0.02453536901606883.\n",
      "Training on epoch 1 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693185, and regularization loss is 3.8e-05.\n",
      " Top K precision = 0.11157894736842097, recall = 0.024098999701889032.\n",
      "Training on epoch 1 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693172, and regularization loss is 2.5e-05.\n",
      " Top K precision = 0.09479166666666665, recall = 0.02598364636956392.\n",
      "Training on epoch 1 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693187, and regularization loss is 4e-05.\n",
      " Top K precision = 0.10721649484536075, recall = 0.029133319260284667.\n",
      "Training on epoch 1 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693178, and regularization loss is 3.1e-05.\n",
      " Top K precision = 0.08877551020408159, recall = 0.02742506371745717.\n",
      "Training on epoch 1 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693175, and regularization loss is 2.8e-05.\n",
      " Top K precision = 0.1061855670103092, recall = 0.025629948362572087.\n",
      "Training on epoch 1 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.69318, and regularization loss is 3.3e-05.\n",
      " Top K precision = 0.1302083333333333, recall = 0.026548426132094113.\n",
      "Training on epoch 1 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693178, and regularization loss is 3.1e-05.\n",
      " Top K precision = 0.10531914893617014, recall = 0.029135438864576927.\n",
      "Training on epoch 1 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693197, and regularization loss is 5e-05.\n",
      " Top K precision = 0.12580645161290313, recall = 0.02236989733303404.\n",
      "\n",
      "Training on 1 epoch completed.\n",
      " Average bpr_loss on train set is 0.006932 for the current epoch.\n",
      " Training top K precision = 0.09172852598091193, recall = 0.029529829747232304.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.09172852598091186, recall = 0.02952982974723228.\n",
      "\n",
      "Training on the 2 epoch\n",
      "Training on epoch 2 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693189, and regularization loss is 4.2e-05.\n",
      " Top K precision = 0.08817204301075264, recall = 0.019590805771789475.\n",
      "Training on epoch 2 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693195, and regularization loss is 4.7e-05.\n",
      " Top K precision = 0.11458333333333327, recall = 0.022236699993415942.\n",
      "Training on epoch 2 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693191, and regularization loss is 4.3e-05.\n",
      " Top K precision = 0.12365591397849451, recall = 0.028257246370375078.\n",
      "Training on epoch 2 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693175, and regularization loss is 2.8e-05.\n",
      " Top K precision = 0.10957446808510637, recall = 0.026268550673991226.\n",
      "Training on epoch 2 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693186, and regularization loss is 3.9e-05.\n",
      " Top K precision = 0.11145833333333326, recall = 0.028664148247037075.\n",
      "Training on epoch 2 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693192, and regularization loss is 4.5e-05.\n",
      " Top K precision = 0.10967741935483867, recall = 0.03096638208706904.\n",
      "Training on epoch 2 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693191, and regularization loss is 4.3e-05.\n",
      " Top K precision = 0.09684210526315783, recall = 0.025421229288996632.\n",
      "Training on epoch 2 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693206, and regularization loss is 5.9e-05.\n",
      " Top K precision = 0.09999999999999996, recall = 0.021560526379128843.\n",
      "Training on epoch 2 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693197, and regularization loss is 5e-05.\n",
      " Top K precision = 0.10526315789473675, recall = 0.024338454852110513.\n",
      "Training on epoch 2 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.693198, and regularization loss is 5.1e-05.\n",
      " Top K precision = 0.09473684210526316, recall = 0.01862111792278864.\n",
      "Training on epoch 2 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693204, and regularization loss is 5.7e-05.\n",
      " Top K precision = 0.12577319587628857, recall = 0.028615236533823706.\n",
      "Training on epoch 2 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693203, and regularization loss is 5.6e-05.\n",
      " Top K precision = 0.10918367346938768, recall = 0.02917478846599981.\n",
      "Training on epoch 2 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693199, and regularization loss is 5.2e-05.\n",
      " Top K precision = 0.10879120879120875, recall = 0.0272933997518988.\n",
      "Training on epoch 2 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693207, and regularization loss is 5.9e-05.\n",
      " Top K precision = 0.10520833333333328, recall = 0.031594102863667754.\n",
      "Training on epoch 2 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693203, and regularization loss is 5.6e-05.\n",
      " Top K precision = 0.11368421052631573, recall = 0.03258529515632897.\n",
      "Training on epoch 2 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693198, and regularization loss is 5.1e-05.\n",
      " Top K precision = 0.11827956989247305, recall = 0.028376748672176166.\n",
      "Training on epoch 2 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.69326, and regularization loss is 0.000113.\n",
      " Top K precision = 0.09896907216494841, recall = 0.02280324665680231.\n",
      "Training on epoch 2 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693211, and regularization loss is 6.4e-05.\n",
      " Top K precision = 0.12421052631578938, recall = 0.0245707747748108.\n",
      "Training on epoch 2 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693211, and regularization loss is 6.4e-05.\n",
      " Top K precision = 0.10106382978723398, recall = 0.021528186932011913.\n",
      "\n",
      "Training on 2 epoch completed.\n",
      " Average bpr_loss on train set is 0.006932 for the current epoch.\n",
      " Training top K precision = 0.0920466595970306, recall = 0.029894936746360866.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.09172852598091186, recall = 0.02952982974723228.\n",
      "\n",
      "Training on the 3 epoch\n",
      "Training on epoch 3 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693211, and regularization loss is 6.4e-05.\n",
      " Top K precision = 0.10206185567010302, recall = 0.024389909408466214.\n",
      "Training on epoch 3 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.69321, and regularization loss is 6.2e-05.\n",
      " Top K precision = 0.11999999999999995, recall = 0.03265950966373934.\n",
      "Training on epoch 3 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693205, and regularization loss is 5.8e-05.\n",
      " Top K precision = 0.13711340206185554, recall = 0.022729612928466737.\n",
      "Training on epoch 3 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693213, and regularization loss is 6.6e-05.\n",
      " Top K precision = 0.12631578947368416, recall = 0.024474631991877147.\n",
      "Training on epoch 3 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693226, and regularization loss is 7.9e-05.\n",
      " Top K precision = 0.1010416666666666, recall = 0.02217902149919579.\n",
      "Training on epoch 3 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693248, and regularization loss is 0.000101.\n",
      " Top K precision = 0.11263157894736832, recall = 0.022731089956351803.\n",
      "Training on epoch 3 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693221, and regularization loss is 7.4e-05.\n",
      " Top K precision = 0.10416666666666662, recall = 0.025347503647816558.\n",
      "Training on epoch 3 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693225, and regularization loss is 7.8e-05.\n",
      " Top K precision = 0.12040816326530608, recall = 0.028928628626175235.\n",
      "Training on epoch 3 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693205, and regularization loss is 5.8e-05.\n",
      " Top K precision = 0.09489795918367343, recall = 0.02820097673296445.\n",
      "Training on epoch 3 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.693256, and regularization loss is 0.000109.\n",
      " Top K precision = 0.13402061855670097, recall = 0.029392983565976766.\n",
      "Training on epoch 3 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693261, and regularization loss is 0.000114.\n",
      " Top K precision = 0.10531914893617014, recall = 0.027081829699618434.\n",
      "Training on epoch 3 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693265, and regularization loss is 0.000118.\n",
      " Top K precision = 0.12021276595744676, recall = 0.026212776884377398.\n",
      "Training on epoch 3 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693243, and regularization loss is 9.6e-05.\n",
      " Top K precision = 0.11562499999999996, recall = 0.02297809908491993.\n",
      "Training on epoch 3 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693253, and regularization loss is 0.000106.\n",
      " Top K precision = 0.1302083333333333, recall = 0.03006049248337446.\n",
      "Training on epoch 3 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.69323, and regularization loss is 8.2e-05.\n",
      " Top K precision = 0.11382978723404247, recall = 0.03201399674401884.\n",
      "Training on epoch 3 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693231, and regularization loss is 8.4e-05.\n",
      " Top K precision = 0.11249999999999995, recall = 0.026982833200729153.\n",
      "Training on epoch 3 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693244, and regularization loss is 9.7e-05.\n",
      " Top K precision = 0.1145833333333333, recall = 0.021005430113669423.\n",
      "Training on epoch 3 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693267, and regularization loss is 0.00012.\n",
      " Top K precision = 0.09894736842105256, recall = 0.028398913550668085.\n",
      "Training on epoch 3 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693259, and regularization loss is 0.000112.\n",
      " Top K precision = 0.1187499999999999, recall = 0.033277109417421805.\n",
      "\n",
      "Training on 3 epoch completed.\n",
      " Average bpr_loss on train set is 0.006932 for the current epoch.\n",
      " Training top K precision = 0.09119830328738056, recall = 0.029476815827284193.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.09162248144220561, recall = 0.0295601989937496.\n",
      "\n",
      "Training on the 4 epoch\n",
      "Training on epoch 4 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693241, and regularization loss is 9.3e-05.\n",
      " Top K precision = 0.10104166666666663, recall = 0.021407933864801932.\n",
      "Training on epoch 4 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693329, and regularization loss is 0.000182.\n",
      " Top K precision = 0.11538461538461531, recall = 0.028953221594775126.\n",
      "Training on epoch 4 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693392, and regularization loss is 0.000245.\n",
      " Top K precision = 0.11666666666666657, recall = 0.026530987067589242.\n",
      "Training on epoch 4 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693293, and regularization loss is 0.000145.\n",
      " Top K precision = 0.07684210526315786, recall = 0.02374229623353164.\n",
      "Training on epoch 4 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693273, and regularization loss is 0.000126.\n",
      " Top K precision = 0.10215053763440857, recall = 0.024335409120203877.\n",
      "Training on epoch 4 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693292, and regularization loss is 0.000145.\n",
      " Top K precision = 0.1123711340206185, recall = 0.03496496105631139.\n",
      "Training on epoch 4 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693297, and regularization loss is 0.00015.\n",
      " Top K precision = 0.11910112359550558, recall = 0.022134785557560075.\n",
      "Training on epoch 4 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693317, and regularization loss is 0.000169.\n",
      " Top K precision = 0.11855670103092776, recall = 0.027553187002590546.\n",
      "Training on epoch 4 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693289, and regularization loss is 0.000142.\n",
      " Top K precision = 0.12105263157894731, recall = 0.02540280391329513.\n",
      "Training on epoch 4 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.693258, and regularization loss is 0.00011.\n",
      " Top K precision = 0.09895833333333331, recall = 0.021483627127507062.\n",
      "Training on epoch 4 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693255, and regularization loss is 0.000108.\n",
      " Top K precision = 0.0959183673469387, recall = 0.022577295827906286.\n",
      "Training on epoch 4 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693246, and regularization loss is 9.9e-05.\n",
      " Top K precision = 0.11666666666666659, recall = 0.026937002221051613.\n",
      "Training on epoch 4 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693309, and regularization loss is 0.000162.\n",
      " Top K precision = 0.10408163265306117, recall = 0.02770545178211849.\n",
      "Training on epoch 4 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693287, and regularization loss is 0.00014.\n",
      " Top K precision = 0.10106382978723398, recall = 0.029477412573658764.\n",
      "Training on epoch 4 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693317, and regularization loss is 0.00017.\n",
      " Top K precision = 0.1273684210526315, recall = 0.030365389968166106.\n",
      "Training on epoch 4 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693318, and regularization loss is 0.000171.\n",
      " Top K precision = 0.09687499999999998, recall = 0.022759643722385595.\n",
      "Training on epoch 4 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693264, and regularization loss is 0.000117.\n",
      " Top K precision = 0.10416666666666656, recall = 0.031051412818936905.\n",
      "Training on epoch 4 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693312, and regularization loss is 0.000165.\n",
      " Top K precision = 0.09381443298969067, recall = 0.030417507915303916.\n",
      "Training on epoch 4 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693322, and regularization loss is 0.000175.\n",
      " Top K precision = 0.09999999999999996, recall = 0.022889650191464748.\n",
      "\n",
      "Training on 4 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.09098621420996815, recall = 0.029027226024692043.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.09109225874867431, recall = 0.028862648375483996.\n",
      "\n",
      "Training on the 5 epoch\n",
      "Training on epoch 5 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693286, and regularization loss is 0.000139.\n",
      " Top K precision = 0.09381443298969064, recall = 0.023498384054030045.\n",
      "Training on epoch 5 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693313, and regularization loss is 0.000166.\n",
      " Top K precision = 0.09032258064516124, recall = 0.025099017451080972.\n",
      "Training on epoch 5 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693273, and regularization loss is 0.000126.\n",
      " Top K precision = 0.11368421052631572, recall = 0.02061382794806535.\n",
      "Training on epoch 5 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693304, and regularization loss is 0.000157.\n",
      " Top K precision = 0.11578947368421046, recall = 0.03434150625271324.\n",
      "Training on epoch 5 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693305, and regularization loss is 0.000158.\n",
      " Top K precision = 0.11030927835051542, recall = 0.023019767292074472.\n",
      "Training on epoch 5 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693268, and regularization loss is 0.000121.\n",
      " Top K precision = 0.1030927835051546, recall = 0.029067732186575823.\n",
      "Training on epoch 5 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693319, and regularization loss is 0.000171.\n",
      " Top K precision = 0.11702127659574463, recall = 0.019945295703708194.\n",
      "Training on epoch 5 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693358, and regularization loss is 0.000211.\n",
      " Top K precision = 0.10312499999999993, recall = 0.020964442841669675.\n",
      "Training on epoch 5 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.6933, and regularization loss is 0.000153.\n",
      " Top K precision = 0.09374999999999996, recall = 0.02015238954206776.\n",
      "Training on epoch 5 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.69327, and regularization loss is 0.000123.\n",
      " Top K precision = 0.10326086956521738, recall = 0.03401395239304394.\n",
      "Training on epoch 5 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693333, and regularization loss is 0.000185.\n",
      " Top K precision = 0.10104166666666663, recall = 0.02906563021025647.\n",
      "Training on epoch 5 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693294, and regularization loss is 0.000147.\n",
      " Top K precision = 0.09893617021276595, recall = 0.02696247244368979.\n",
      "Training on epoch 5 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.69327, and regularization loss is 0.000123.\n",
      " Top K precision = 0.11473684210526307, recall = 0.02483530881034272.\n",
      "Training on epoch 5 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693316, and regularization loss is 0.000169.\n",
      " Top K precision = 0.1104166666666666, recall = 0.02290153277580716.\n",
      "Training on epoch 5 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693317, and regularization loss is 0.00017.\n",
      " Top K precision = 0.10204081632653056, recall = 0.026728578507399434.\n",
      "Training on epoch 5 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693285, and regularization loss is 0.000138.\n",
      " Top K precision = 0.11562499999999992, recall = 0.02893484625982352.\n",
      "Training on epoch 5 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693283, and regularization loss is 0.000136.\n",
      " Top K precision = 0.11263157894736833, recall = 0.025099440730850346.\n",
      "Training on epoch 5 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693311, and regularization loss is 0.000164.\n",
      " Top K precision = 0.12087912087912084, recall = 0.030028587406821034.\n",
      "Training on epoch 5 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693314, and regularization loss is 0.000167.\n",
      " Top K precision = 0.10208333333333326, recall = 0.02569035193362092.\n",
      "\n",
      "Training on 5 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.09194061505832436, recall = 0.029020601014007576.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.09045599151643675, recall = 0.029005086652107503.\n",
      "\n",
      "Training on the 6 epoch\n",
      "Training on epoch 6 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693298, and regularization loss is 0.00015.\n",
      " Top K precision = 0.11333333333333324, recall = 0.022599475874489697.\n",
      "Training on epoch 6 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693294, and regularization loss is 0.000147.\n",
      " Top K precision = 0.11157894736842103, recall = 0.02024980856858564.\n",
      "Training on epoch 6 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693288, and regularization loss is 0.000141.\n",
      " Top K precision = 0.11276595744680845, recall = 0.028155530534653785.\n",
      "Training on epoch 6 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693288, and regularization loss is 0.00014.\n",
      " Top K precision = 0.09893617021276593, recall = 0.022616844954944832.\n",
      "Training on epoch 6 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693273, and regularization loss is 0.000126.\n",
      " Top K precision = 0.1285714285714285, recall = 0.033015160032681616.\n",
      "Training on epoch 6 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693328, and regularization loss is 0.000181.\n",
      " Top K precision = 0.083695652173913, recall = 0.024956791113634712.\n",
      "Training on epoch 6 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693278, and regularization loss is 0.000131.\n",
      " Top K precision = 0.10312499999999997, recall = 0.022695364614383373.\n",
      "Training on epoch 6 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693266, and regularization loss is 0.000119.\n",
      " Top K precision = 0.09062499999999997, recall = 0.021247063593792994.\n",
      "Training on epoch 6 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693299, and regularization loss is 0.000152.\n",
      " Top K precision = 0.1208791208791208, recall = 0.021968103272116044.\n",
      "Training on epoch 6 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.693262, and regularization loss is 0.000115.\n",
      " Top K precision = 0.12577319587628857, recall = 0.025602624955415095.\n",
      "Training on epoch 6 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693305, and regularization loss is 0.000157.\n",
      " Top K precision = 0.09673913043478258, recall = 0.024549019404445847.\n",
      "Training on epoch 6 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693305, and regularization loss is 0.000158.\n",
      " Top K precision = 0.12708333333333324, recall = 0.029979723679011198.\n",
      "Training on epoch 6 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.69327, and regularization loss is 0.000123.\n",
      " Top K precision = 0.1151515151515151, recall = 0.026042663432674713.\n",
      "Training on epoch 6 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.69326, and regularization loss is 0.000113.\n",
      " Top K precision = 0.12258064516129027, recall = 0.028132073767442763.\n",
      "Training on epoch 6 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693275, and regularization loss is 0.000128.\n",
      " Top K precision = 0.07708333333333332, recall = 0.022646573677346696.\n",
      "Training on epoch 6 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.69331, and regularization loss is 0.000163.\n",
      " Top K precision = 0.1311827956989247, recall = 0.031992016166205284.\n",
      "Training on epoch 6 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693266, and regularization loss is 0.000118.\n",
      " Top K precision = 0.10631578947368413, recall = 0.02884939130935132.\n",
      "Training on epoch 6 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693297, and regularization loss is 0.00015.\n",
      " Top K precision = 0.08958333333333325, recall = 0.02345780028611497.\n",
      "Training on epoch 6 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693295, and regularization loss is 0.000148.\n",
      " Top K precision = 0.10104166666666663, recall = 0.021113427770179143.\n",
      "\n",
      "Training on 6 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.09130434782608678, recall = 0.029295724850495877.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.08950159066808044, recall = 0.028843825219675816.\n",
      "\n",
      "Training on the 7 epoch\n",
      "Training on epoch 7 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693312, and regularization loss is 0.000165.\n",
      " Top K precision = 0.10425531914893615, recall = 0.016392263325644713.\n",
      "Training on epoch 7 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693281, and regularization loss is 0.000133.\n",
      " Top K precision = 0.10736842105263152, recall = 0.02623738850610012.\n",
      "Training on epoch 7 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693319, and regularization loss is 0.000172.\n",
      " Top K precision = 0.12473118279569882, recall = 0.023035774162544938.\n",
      "Training on epoch 7 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.6933, and regularization loss is 0.000152.\n",
      " Top K precision = 0.10957446808510635, recall = 0.019889567944895322.\n",
      "Training on epoch 7 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693267, and regularization loss is 0.00012.\n",
      " Top K precision = 0.10312499999999997, recall = 0.020574735681979445.\n",
      "Training on epoch 7 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693302, and regularization loss is 0.000154.\n",
      " Top K precision = 0.09780219780219775, recall = 0.019856647849891496.\n",
      "Training on epoch 7 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693298, and regularization loss is 0.000151.\n",
      " Top K precision = 0.11413043478260862, recall = 0.03243121168950638.\n",
      "Training on epoch 7 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693301, and regularization loss is 0.000154.\n",
      " Top K precision = 0.10520833333333325, recall = 0.023523240525590475.\n",
      "Training on epoch 7 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693287, and regularization loss is 0.00014.\n",
      " Top K precision = 0.11041666666666665, recall = 0.02737726610087707.\n",
      "Training on epoch 7 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.693279, and regularization loss is 0.000132.\n",
      " Top K precision = 0.09673913043478259, recall = 0.021429103922511804.\n",
      "Training on epoch 7 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.69331, and regularization loss is 0.000163.\n",
      " Top K precision = 0.09894736842105262, recall = 0.023056381478236403.\n",
      "Training on epoch 7 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693297, and regularization loss is 0.00015.\n",
      " Top K precision = 0.11354166666666661, recall = 0.02833954729537567.\n",
      "Training on epoch 7 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.69326, and regularization loss is 0.000113.\n",
      " Top K precision = 0.10210526315789473, recall = 0.027188622258112378.\n",
      "Training on epoch 7 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693368, and regularization loss is 0.000221.\n",
      " Top K precision = 0.10224719101123592, recall = 0.025359220188826952.\n",
      "Training on epoch 7 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.69331, and regularization loss is 0.000163.\n",
      " Top K precision = 0.12307692307692296, recall = 0.024484765184711862.\n",
      "Training on epoch 7 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693294, and regularization loss is 0.000147.\n",
      " Top K precision = 0.11263157894736836, recall = 0.026349887861961126.\n",
      "Training on epoch 7 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693289, and regularization loss is 0.000142.\n",
      " Top K precision = 0.12061855670103083, recall = 0.023993647006599342.\n",
      "Training on epoch 7 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.69328, and regularization loss is 0.000133.\n",
      " Top K precision = 0.12282608695652167, recall = 0.03274780774414869.\n",
      "Training on epoch 7 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693317, and regularization loss is 0.00017.\n",
      " Top K precision = 0.11182795698924725, recall = 0.026220167684205178.\n",
      "\n",
      "Training on 7 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.09257688229056196, recall = 0.02979180746317743.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.08865323435843041, recall = 0.028282100244782088.\n",
      "\n",
      "Training on the 8 epoch\n",
      "Training on epoch 8 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693291, and regularization loss is 0.000144.\n",
      " Top K precision = 0.11157894736842101, recall = 0.026647681396336838.\n",
      "Training on epoch 8 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.69325, and regularization loss is 0.000103.\n",
      " Top K precision = 0.1030927835051546, recall = 0.01945581173217344.\n",
      "Training on epoch 8 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693291, and regularization loss is 0.000143.\n",
      " Top K precision = 0.10729166666666656, recall = 0.025140836411222448.\n",
      "Training on epoch 8 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693279, and regularization loss is 0.000132.\n",
      " Top K precision = 0.11145833333333326, recall = 0.024800695877930854.\n",
      "Training on epoch 8 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693317, and regularization loss is 0.00017.\n",
      " Top K precision = 0.11666666666666659, recall = 0.03955977859404272.\n",
      "Training on epoch 8 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693291, and regularization loss is 0.000143.\n",
      " Top K precision = 0.0947368421052631, recall = 0.02292678789804646.\n",
      "Training on epoch 8 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693312, and regularization loss is 0.000165.\n",
      " Top K precision = 0.11290322580645155, recall = 0.024328990213144712.\n",
      "Training on epoch 8 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693325, and regularization loss is 0.000178.\n",
      " Top K precision = 0.10103092783505149, recall = 0.024847431099325586.\n",
      "Training on epoch 8 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693316, and regularization loss is 0.000169.\n",
      " Top K precision = 0.12268041237113397, recall = 0.024404529606504938.\n",
      "Training on epoch 8 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.69326, and regularization loss is 0.000113.\n",
      " Top K precision = 0.10957446808510632, recall = 0.02666170870653335.\n",
      "Training on epoch 8 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693255, and regularization loss is 0.000107.\n",
      " Top K precision = 0.0947368421052631, recall = 0.02636191106609442.\n",
      "Training on epoch 8 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693267, and regularization loss is 0.00012.\n",
      " Top K precision = 0.12989690721649474, recall = 0.0347042115116907.\n",
      "Training on epoch 8 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693321, and regularization loss is 0.000173.\n",
      " Top K precision = 0.08804347826086954, recall = 0.026217238095089874.\n",
      "Training on epoch 8 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693273, and regularization loss is 0.000126.\n",
      " Top K precision = 0.10842105263157888, recall = 0.02786010987348581.\n",
      "Training on epoch 8 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693296, and regularization loss is 0.000149.\n",
      " Top K precision = 0.09166666666666663, recall = 0.02956858508989488.\n",
      "Training on epoch 8 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693277, and regularization loss is 0.00013.\n",
      " Top K precision = 0.0978260869565217, recall = 0.028742258833101587.\n",
      "Training on epoch 8 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693303, and regularization loss is 0.000156.\n",
      " Top K precision = 0.10425531914893613, recall = 0.02399227231915408.\n",
      "Training on epoch 8 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693294, and regularization loss is 0.000147.\n",
      " Top K precision = 0.12999999999999992, recall = 0.030411014203560283.\n",
      "Training on epoch 8 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693323, and regularization loss is 0.000175.\n",
      " Top K precision = 0.10816326530612239, recall = 0.022215568820103452.\n",
      "\n",
      "Training on 8 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.09013785790031802, recall = 0.02874299884112992.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.09183457051961812, recall = 0.030059353096958896.\n",
      "\n",
      "Training on the 9 epoch\n",
      "Training on epoch 9 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693299, and regularization loss is 0.000152.\n",
      " Top K precision = 0.10736842105263153, recall = 0.02550108929297193.\n",
      "Training on epoch 9 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693318, and regularization loss is 0.000171.\n",
      " Top K precision = 0.12268041237113399, recall = 0.02256078148827411.\n",
      "Training on epoch 9 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693304, and regularization loss is 0.000157.\n",
      " Top K precision = 0.10957446808510632, recall = 0.026553776845999737.\n",
      "Training on epoch 9 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693313, and regularization loss is 0.000165.\n",
      " Top K precision = 0.10208333333333332, recall = 0.01892169747161005.\n",
      "Training on epoch 9 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693281, and regularization loss is 0.000134.\n",
      " Top K precision = 0.09285714285714282, recall = 0.026515241089972198.\n",
      "Training on epoch 9 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693293, and regularization loss is 0.000146.\n",
      " Top K precision = 0.09347826086956514, recall = 0.024144566354234218.\n",
      "Training on epoch 9 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693275, and regularization loss is 0.000128.\n",
      " Top K precision = 0.11276595744680844, recall = 0.025107249120702398.\n",
      "Training on epoch 9 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693275, and regularization loss is 0.000128.\n",
      " Top K precision = 0.10322580645161285, recall = 0.024033578371815953.\n",
      "Training on epoch 9 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693295, and regularization loss is 0.000148.\n",
      " Top K precision = 0.11340206185567, recall = 0.030420168232781545.\n",
      "Training on epoch 9 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.693324, and regularization loss is 0.000177.\n",
      " Top K precision = 0.1063829787234042, recall = 0.028619456090281364.\n",
      "Training on epoch 9 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693295, and regularization loss is 0.000148.\n",
      " Top K precision = 0.11684210526315782, recall = 0.03273386946556806.\n",
      "Training on epoch 9 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693275, and regularization loss is 0.000128.\n",
      " Top K precision = 0.11979166666666662, recall = 0.02628968462493785.\n",
      "Training on epoch 9 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693344, and regularization loss is 0.000197.\n",
      " Top K precision = 0.12282608695652163, recall = 0.02861137756105685.\n",
      "Training on epoch 9 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693346, and regularization loss is 0.000199.\n",
      " Top K precision = 0.10842105263157888, recall = 0.026591586977631223.\n",
      "Training on epoch 9 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693301, and regularization loss is 0.000154.\n",
      " Top K precision = 0.11935483870967738, recall = 0.02384242477579201.\n",
      "Training on epoch 9 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693347, and regularization loss is 0.0002.\n",
      " Top K precision = 0.10638297872340421, recall = 0.02212394068994899.\n",
      "Training on epoch 9 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693275, and regularization loss is 0.000128.\n",
      " Top K precision = 0.10842105263157888, recall = 0.02573484306816794.\n",
      "Training on epoch 9 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693306, and regularization loss is 0.000159.\n",
      " Top K precision = 0.10416666666666667, recall = 0.01756768433971233.\n",
      "Training on epoch 9 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693286, and regularization loss is 0.000139.\n",
      " Top K precision = 0.09680851063829787, recall = 0.029536184790833513.\n",
      "\n",
      "Training on 9 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.09183457051961814, recall = 0.029195563603961094.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.0911983032873806, recall = 0.028915069449964356.\n",
      "\n",
      "Training on the 10 epoch\n",
      "Training on epoch 10 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.69326, and regularization loss is 0.000112.\n",
      " Top K precision = 0.10210526315789471, recall = 0.03272992248736588.\n",
      "Training on epoch 10 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693294, and regularization loss is 0.000147.\n",
      " Top K precision = 0.09052631578947365, recall = 0.019004311739925693.\n",
      "Training on epoch 10 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693297, and regularization loss is 0.00015.\n",
      " Top K precision = 0.12282608695652165, recall = 0.02578926048309132.\n",
      "Training on epoch 10 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693296, and regularization loss is 0.000148.\n",
      " Top K precision = 0.12999999999999995, recall = 0.02840440396594244.\n",
      "Training on epoch 10 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693293, and regularization loss is 0.000146.\n",
      " Top K precision = 0.11827956989247305, recall = 0.027515123991832507.\n",
      "Training on epoch 10 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693294, and regularization loss is 0.000146.\n",
      " Top K precision = 0.11313131313131307, recall = 0.02936919071884496.\n",
      "Training on epoch 10 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693258, and regularization loss is 0.00011.\n",
      " Top K precision = 0.11157894736842099, recall = 0.023508058586428654.\n",
      "Training on epoch 10 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693303, and regularization loss is 0.000156.\n",
      " Top K precision = 0.12474226804123707, recall = 0.025896074010643883.\n",
      "Training on epoch 10 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693264, and regularization loss is 0.000117.\n",
      " Top K precision = 0.12499999999999996, recall = 0.02871983041990624.\n",
      "Training on epoch 10 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.693273, and regularization loss is 0.000126.\n",
      " Top K precision = 0.10744680851063823, recall = 0.03278089288139262.\n",
      "Training on epoch 10 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693301, and regularization loss is 0.000154.\n",
      " Top K precision = 0.07755102040816324, recall = 0.018255529257418936.\n",
      "Training on epoch 10 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.69335, and regularization loss is 0.000203.\n",
      " Top K precision = 0.11666666666666657, recall = 0.02987329654143008.\n",
      "Training on epoch 10 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693275, and regularization loss is 0.000127.\n",
      " Top K precision = 0.11919191919191913, recall = 0.026994904954606633.\n",
      "Training on epoch 10 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.69328, and regularization loss is 0.000133.\n",
      " Top K precision = 0.10744680851063823, recall = 0.03038386657203264.\n",
      "Training on epoch 10 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693288, and regularization loss is 0.000141.\n",
      " Top K precision = 0.10537634408602144, recall = 0.025597930219479523.\n",
      "Training on epoch 10 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693267, and regularization loss is 0.00012.\n",
      " Top K precision = 0.08709677419354839, recall = 0.018177208296313682.\n",
      "Training on epoch 10 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.69332, and regularization loss is 0.000173.\n",
      " Top K precision = 0.1135416666666666, recall = 0.0276627704150697.\n",
      "Training on epoch 10 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693245, and regularization loss is 9.8e-05.\n",
      " Top K precision = 0.10947368421052625, recall = 0.02687645367330189.\n",
      "Training on epoch 10 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.69329, and regularization loss is 0.000143.\n",
      " Top K precision = 0.11458333333333325, recall = 0.030039626263892966.\n",
      "\n",
      "Training on 10 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.0907741251325555, recall = 0.02938869151098447.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.09109225874867434, recall = 0.029368424478276957.\n",
      "\n",
      "Training on the 11 epoch\n",
      "Training on epoch 11 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693302, and regularization loss is 0.000155.\n",
      " Top K precision = 0.12187499999999997, recall = 0.023877277466982223.\n",
      "Training on epoch 11 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693285, and regularization loss is 0.000138.\n",
      " Top K precision = 0.09999999999999999, recall = 0.021982332171968582.\n",
      "Training on epoch 11 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693291, and regularization loss is 0.000144.\n",
      " Top K precision = 0.12083333333333325, recall = 0.029009190299475494.\n",
      "Training on epoch 11 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693288, and regularization loss is 0.00014.\n",
      " Top K precision = 0.13191489361702116, recall = 0.03134483023337577.\n",
      "Training on epoch 11 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693319, and regularization loss is 0.000172.\n",
      " Top K precision = 0.10105263157894727, recall = 0.0225337146128243.\n",
      "Training on epoch 11 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.69327, and regularization loss is 0.000123.\n",
      " Top K precision = 0.10206185567010302, recall = 0.030391440219121435.\n",
      "Training on epoch 11 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693357, and regularization loss is 0.00021.\n",
      " Top K precision = 0.08541666666666665, recall = 0.023202474261794493.\n",
      "Training on epoch 11 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.69327, and regularization loss is 0.000123.\n",
      " Top K precision = 0.12371134020618546, recall = 0.026354652641607686.\n",
      "Training on epoch 11 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693301, and regularization loss is 0.000154.\n",
      " Top K precision = 0.1123711340206185, recall = 0.031992910378946464.\n",
      "Training on epoch 11 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.69331, and regularization loss is 0.000163.\n",
      " Top K precision = 0.10319148936170208, recall = 0.025196525305634525.\n",
      "Training on epoch 11 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693319, and regularization loss is 0.000172.\n",
      " Top K precision = 0.13369565217391297, recall = 0.02645020979287006.\n",
      "Training on epoch 11 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693281, and regularization loss is 0.000134.\n",
      " Top K precision = 0.1133333333333333, recall = 0.025256812603717545.\n",
      "Training on epoch 11 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693328, and regularization loss is 0.00018.\n",
      " Top K precision = 0.09578947368421045, recall = 0.02342355202026304.\n",
      "Training on epoch 11 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693284, and regularization loss is 0.000137.\n",
      " Top K precision = 0.1290322580645161, recall = 0.02901774292632119.\n",
      "Training on epoch 11 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693291, and regularization loss is 0.000144.\n",
      " Top K precision = 0.09895833333333326, recall = 0.030180637981368316.\n",
      "Training on epoch 11 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693281, and regularization loss is 0.000134.\n",
      " Top K precision = 0.11649484536082469, recall = 0.03262870728113369.\n",
      "Training on epoch 11 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693297, and regularization loss is 0.00015.\n",
      " Top K precision = 0.11354166666666661, recall = 0.02912425717621707.\n",
      "Training on epoch 11 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.69333, and regularization loss is 0.000183.\n",
      " Top K precision = 0.11808510638297869, recall = 0.02352276578204913.\n",
      "Training on epoch 11 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693253, and regularization loss is 0.000105.\n",
      " Top K precision = 0.10531914893617018, recall = 0.02666472127994498.\n",
      "\n",
      "Training on 11 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.08928950159066804, recall = 0.028881200807277855.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.0879109225874866, recall = 0.027546496212584735.\n",
      "\n",
      "Training on the 12 epoch\n",
      "Training on epoch 12 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693357, and regularization loss is 0.00021.\n",
      " Top K precision = 0.1163043478260869, recall = 0.024201848118979803.\n",
      "Training on epoch 12 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693354, and regularization loss is 0.000207.\n",
      " Top K precision = 0.08124999999999995, recall = 0.023750345426187838.\n",
      "Training on epoch 12 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693292, and regularization loss is 0.000145.\n",
      " Top K precision = 0.12371134020618549, recall = 0.029261479090839752.\n",
      "Training on epoch 12 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693258, and regularization loss is 0.000111.\n",
      " Top K precision = 0.07741935483870963, recall = 0.024510112581823132.\n",
      "Training on epoch 12 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693355, and regularization loss is 0.000208.\n",
      " Top K precision = 0.10957446808510632, recall = 0.02402809231046755.\n",
      "Training on epoch 12 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693366, and regularization loss is 0.000219.\n",
      " Top K precision = 0.09791666666666661, recall = 0.024507409797115565.\n",
      "Training on epoch 12 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693272, and regularization loss is 0.000125.\n",
      " Top K precision = 0.08762886597938141, recall = 0.018128430679444085.\n",
      "Training on epoch 12 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693297, and regularization loss is 0.000149.\n",
      " Top K precision = 0.08749999999999995, recall = 0.028771824817199785.\n",
      "Training on epoch 12 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693276, and regularization loss is 0.000129.\n",
      " Top K precision = 0.10319148936170207, recall = 0.029307017819527912.\n",
      "Training on epoch 12 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.693283, and regularization loss is 0.000136.\n",
      " Top K precision = 0.11808510638297866, recall = 0.026874293270696854.\n",
      "Training on epoch 12 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693303, and regularization loss is 0.000155.\n",
      " Top K precision = 0.11041666666666661, recall = 0.01863254746025453.\n",
      "Training on epoch 12 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693288, and regularization loss is 0.00014.\n",
      " Top K precision = 0.10927835051546383, recall = 0.02710597133307095.\n",
      "Training on epoch 12 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693283, and regularization loss is 0.000135.\n",
      " Top K precision = 0.11354166666666665, recall = 0.031471995876665104.\n",
      "Training on epoch 12 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693303, and regularization loss is 0.000155.\n",
      " Top K precision = 0.12065217391304342, recall = 0.024450494577360666.\n",
      "Training on epoch 12 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693274, and regularization loss is 0.000126.\n",
      " Top K precision = 0.10421052631578942, recall = 0.024583726229254536.\n",
      "Training on epoch 12 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693368, and regularization loss is 0.000221.\n",
      " Top K precision = 0.0989361702127659, recall = 0.02882320005927012.\n",
      "Training on epoch 12 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693323, and regularization loss is 0.000176.\n",
      " Top K precision = 0.12886597938144323, recall = 0.028305146162917235.\n",
      "Training on epoch 12 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693332, and regularization loss is 0.000185.\n",
      " Top K precision = 0.11649484536082466, recall = 0.02845896217046706.\n",
      "Training on epoch 12 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.69328, and regularization loss is 0.000133.\n",
      " Top K precision = 0.11612903225806445, recall = 0.029048486039087912.\n",
      "\n",
      "Training on 12 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.09088016967126175, recall = 0.0290061490921562.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.0916224814422056, recall = 0.029744062389409437.\n",
      "\n",
      "Training on the 13 epoch\n",
      "Training on epoch 13 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693312, and regularization loss is 0.000165.\n",
      " Top K precision = 0.09062499999999996, recall = 0.020904704546746183.\n",
      "Training on epoch 13 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.6933, and regularization loss is 0.000153.\n",
      " Top K precision = 0.10312499999999998, recall = 0.01692388416721333.\n",
      "Training on epoch 13 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693307, and regularization loss is 0.00016.\n",
      " Top K precision = 0.1104166666666666, recall = 0.03084099807097401.\n",
      "Training on epoch 13 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693287, and regularization loss is 0.00014.\n",
      " Top K precision = 0.11122448979591831, recall = 0.02320769341999149.\n",
      "Training on epoch 13 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693282, and regularization loss is 0.000135.\n",
      " Top K precision = 0.11063829787234039, recall = 0.022951448474234474.\n",
      "Training on epoch 13 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.69329, and regularization loss is 0.000142.\n",
      " Top K precision = 0.10212765957446807, recall = 0.021141883051094927.\n",
      "Training on epoch 13 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693298, and regularization loss is 0.00015.\n",
      " Top K precision = 0.12315789473684204, recall = 0.026582885846476043.\n",
      "Training on epoch 13 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693274, and regularization loss is 0.000127.\n",
      " Top K precision = 0.11041666666666661, recall = 0.03188783876032381.\n",
      "Training on epoch 13 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693312, and regularization loss is 0.000165.\n",
      " Top K precision = 0.10421052631578946, recall = 0.025309126782612608.\n",
      "Training on epoch 13 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.693294, and regularization loss is 0.000147.\n",
      " Top K precision = 0.12795698924731175, recall = 0.025027065777239125.\n",
      "Training on epoch 13 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693311, and regularization loss is 0.000164.\n",
      " Top K precision = 0.10421052631578942, recall = 0.027947181227037116.\n",
      "Training on epoch 13 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693303, and regularization loss is 0.000156.\n",
      " Top K precision = 0.12258064516129023, recall = 0.028058472967488056.\n",
      "Training on epoch 13 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693267, and regularization loss is 0.00012.\n",
      " Top K precision = 0.0861702127659574, recall = 0.01751404817858071.\n",
      "Training on epoch 13 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693273, and regularization loss is 0.000126.\n",
      " Top K precision = 0.11249999999999998, recall = 0.020726726183259377.\n",
      "Training on epoch 13 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693288, and regularization loss is 0.000141.\n",
      " Top K precision = 0.11145833333333331, recall = 0.02330268815638203.\n",
      "Training on epoch 13 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693294, and regularization loss is 0.000147.\n",
      " Top K precision = 0.13020833333333326, recall = 0.030159137849873504.\n",
      "Training on epoch 13 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693357, and regularization loss is 0.00021.\n",
      " Top K precision = 0.11684210526315787, recall = 0.033114610646556304.\n",
      "Training on epoch 13 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693297, and regularization loss is 0.00015.\n",
      " Top K precision = 0.10618556701030923, recall = 0.02869588205528035.\n",
      "Training on epoch 13 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693317, and regularization loss is 0.00017.\n",
      " Top K precision = 0.11855670103092776, recall = 0.03162033664793951.\n",
      "\n",
      "Training on 13 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.09109225874867433, recall = 0.029487355281591666.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.09162248144220561, recall = 0.029790825405685454.\n",
      "\n",
      "Training on the 14 epoch\n",
      "Training on epoch 14 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693297, and regularization loss is 0.00015.\n",
      " Top K precision = 0.11443298969072155, recall = 0.02555759607544687.\n",
      "Training on epoch 14 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693308, and regularization loss is 0.00016.\n",
      " Top K precision = 0.09684210526315787, recall = 0.022160930449075834.\n",
      "Training on epoch 14 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693294, and regularization loss is 0.000147.\n",
      " Top K precision = 0.09896907216494838, recall = 0.02349164365446462.\n",
      "Training on epoch 14 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693318, and regularization loss is 0.000171.\n",
      " Top K precision = 0.1354166666666666, recall = 0.028582413348686178.\n",
      "Training on epoch 14 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693271, and regularization loss is 0.000123.\n",
      " Top K precision = 0.08775510204081627, recall = 0.024846048293763225.\n",
      "Training on epoch 14 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693259, and regularization loss is 0.000112.\n",
      " Top K precision = 0.10612244897959175, recall = 0.022777763431721144.\n",
      "Training on epoch 14 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693301, and regularization loss is 0.000154.\n",
      " Top K precision = 0.09368421052631572, recall = 0.035843880023542815.\n",
      "Training on epoch 14 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693285, and regularization loss is 0.000138.\n",
      " Top K precision = 0.09684210526315785, recall = 0.029855554489440533.\n",
      "Training on epoch 14 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693332, and regularization loss is 0.000184.\n",
      " Top K precision = 0.09591836734693876, recall = 0.02149926826935492.\n",
      "Training on epoch 14 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.693282, and regularization loss is 0.000135.\n",
      " Top K precision = 0.10206185567010298, recall = 0.029221198155719476.\n",
      "Training on epoch 14 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693288, and regularization loss is 0.000141.\n",
      " Top K precision = 0.09574468085106379, recall = 0.027641391965717056.\n",
      "Training on epoch 14 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693299, and regularization loss is 0.000152.\n",
      " Top K precision = 0.11340206185567005, recall = 0.02582017463356368.\n",
      "Training on epoch 14 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693351, and regularization loss is 0.000204.\n",
      " Top K precision = 0.1170212765957446, recall = 0.038217757629667205.\n",
      "Training on epoch 14 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693298, and regularization loss is 0.000151.\n",
      " Top K precision = 0.10103092783505149, recall = 0.022110702380441098.\n",
      "Training on epoch 14 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693256, and regularization loss is 0.000108.\n",
      " Top K precision = 0.11086956521739126, recall = 0.024625173962071447.\n",
      "Training on epoch 14 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.69332, and regularization loss is 0.000173.\n",
      " Top K precision = 0.09569892473118276, recall = 0.027775946996217205.\n",
      "Training on epoch 14 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693279, and regularization loss is 0.000131.\n",
      " Top K precision = 0.08315789473684206, recall = 0.023116584854280028.\n",
      "Training on epoch 14 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.69326, and regularization loss is 0.000112.\n",
      " Top K precision = 0.1153061224489795, recall = 0.02562412283839536.\n",
      "Training on epoch 14 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693312, and regularization loss is 0.000165.\n",
      " Top K precision = 0.11935483870967732, recall = 0.026620951601707268.\n",
      "\n",
      "Training on 14 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.0902439024390243, recall = 0.02884439517817848.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.0900318133616118, recall = 0.02909441912080985.\n",
      "\n",
      "Training on the 15 epoch\n",
      "Training on epoch 15 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693346, and regularization loss is 0.000199.\n",
      " Top K precision = 0.11122448979591831, recall = 0.026657715899362878.\n",
      "Training on epoch 15 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693287, and regularization loss is 0.00014.\n",
      " Top K precision = 0.11276595744680852, recall = 0.02668269196019393.\n",
      "Training on epoch 15 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693276, and regularization loss is 0.000128.\n",
      " Top K precision = 0.0760869565217391, recall = 0.026894941632926925.\n",
      "Training on epoch 15 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693301, and regularization loss is 0.000154.\n",
      " Top K precision = 0.09999999999999991, recall = 0.025303760554363643.\n",
      "Training on epoch 15 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693293, and regularization loss is 0.000146.\n",
      " Top K precision = 0.11489361702127648, recall = 0.02659356228775035.\n",
      "Training on epoch 15 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693294, and regularization loss is 0.000147.\n",
      " Top K precision = 0.1322916666666666, recall = 0.03035189891551826.\n",
      "Training on epoch 15 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693328, and regularization loss is 0.000181.\n",
      " Top K precision = 0.0895833333333333, recall = 0.03244528309944863.\n",
      "Training on epoch 15 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693293, and regularization loss is 0.000146.\n",
      " Top K precision = 0.08969072164948447, recall = 0.03334266766974552.\n",
      "Training on epoch 15 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693275, and regularization loss is 0.000128.\n",
      " Top K precision = 0.10729166666666666, recall = 0.024975964882867237.\n",
      "Training on epoch 15 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.693328, and regularization loss is 0.000181.\n",
      " Top K precision = 0.10526315789473678, recall = 0.02174160410839798.\n",
      "Training on epoch 15 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693322, and regularization loss is 0.000175.\n",
      " Top K precision = 0.11263157894736837, recall = 0.028762011668898965.\n",
      "Training on epoch 15 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693277, and regularization loss is 0.00013.\n",
      " Top K precision = 0.10625, recall = 0.0244346442407512.\n",
      "Training on epoch 15 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693298, and regularization loss is 0.000151.\n",
      " Top K precision = 0.10860215053763439, recall = 0.02986596875505047.\n",
      "Training on epoch 15 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693289, and regularization loss is 0.000142.\n",
      " Top K precision = 0.11326530612244894, recall = 0.023499636399292707.\n",
      "Training on epoch 15 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693272, and regularization loss is 0.000124.\n",
      " Top K precision = 0.10957446808510633, recall = 0.028061944736754066.\n",
      "Training on epoch 15 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693252, and regularization loss is 0.000104.\n",
      " Top K precision = 0.12916666666666662, recall = 0.025886389605794636.\n",
      "Training on epoch 15 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693287, and regularization loss is 0.00014.\n",
      " Top K precision = 0.1290322580645161, recall = 0.025540946487265213.\n",
      "Training on epoch 15 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693342, and regularization loss is 0.000195.\n",
      " Top K precision = 0.10714285714285707, recall = 0.024214443768495686.\n",
      "Training on epoch 15 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693287, and regularization loss is 0.00014.\n",
      " Top K precision = 0.09052631578947365, recall = 0.01763597786514765.\n",
      "\n",
      "Training on 15 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.08844114528101797, recall = 0.02860941907641471.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.09045599151643681, recall = 0.030033498606448654.\n",
      "\n",
      "Training on the 16 epoch\n",
      "Training on epoch 16 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693282, and regularization loss is 0.000135.\n",
      " Top K precision = 0.11249999999999993, recall = 0.02774688397975772.\n",
      "Training on epoch 16 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.69329, and regularization loss is 0.000143.\n",
      " Top K precision = 0.10957446808510629, recall = 0.02955194747442965.\n",
      "Training on epoch 16 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693264, and regularization loss is 0.000116.\n",
      " Top K precision = 0.10105263157894732, recall = 0.028439821533100436.\n",
      "Training on epoch 16 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693308, and regularization loss is 0.000161.\n",
      " Top K precision = 0.12580645161290316, recall = 0.02749858985032458.\n",
      "Training on epoch 16 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693269, and regularization loss is 0.000122.\n",
      " Top K precision = 0.10510204081632649, recall = 0.02320076883723316.\n",
      "Training on epoch 16 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693253, and regularization loss is 0.000106.\n",
      " Top K precision = 0.12371134020618552, recall = 0.03559660221614459.\n",
      "Training on epoch 16 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693274, and regularization loss is 0.000127.\n",
      " Top K precision = 0.13617021276595745, recall = 0.021895233159560955.\n",
      "Training on epoch 16 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693324, and regularization loss is 0.000177.\n",
      " Top K precision = 0.09999999999999995, recall = 0.030668304339617387.\n",
      "Training on epoch 16 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693272, and regularization loss is 0.000124.\n",
      " Top K precision = 0.1072916666666666, recall = 0.02346945774757952.\n",
      "Training on epoch 16 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.69335, and regularization loss is 0.000202.\n",
      " Top K precision = 0.11632653061224481, recall = 0.02136105906441211.\n",
      "Training on epoch 16 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693311, and regularization loss is 0.000164.\n",
      " Top K precision = 0.11170212765957441, recall = 0.02727587593466406.\n",
      "Training on epoch 16 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693299, and regularization loss is 0.000151.\n",
      " Top K precision = 0.10104166666666663, recall = 0.026984781832762293.\n",
      "Training on epoch 16 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.69331, and regularization loss is 0.000163.\n",
      " Top K precision = 0.1119565217391304, recall = 0.024536862811058272.\n",
      "Training on epoch 16 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693287, and regularization loss is 0.00014.\n",
      " Top K precision = 0.10851063829787222, recall = 0.02712091077104501.\n",
      "Training on epoch 16 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693282, and regularization loss is 0.000134.\n",
      " Top K precision = 0.10329670329670325, recall = 0.022417147507654556.\n",
      "Training on epoch 16 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.69329, and regularization loss is 0.000142.\n",
      " Top K precision = 0.09999999999999999, recall = 0.02116774186848407.\n",
      "Training on epoch 16 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693293, and regularization loss is 0.000146.\n",
      " Top K precision = 0.12989690721649483, recall = 0.03033967047609442.\n",
      "Training on epoch 16 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693258, and regularization loss is 0.000111.\n",
      " Top K precision = 0.09690721649484532, recall = 0.022749103214238675.\n",
      "Training on epoch 16 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.69329, and regularization loss is 0.000143.\n",
      " Top K precision = 0.11443298969072159, recall = 0.032542429679427914.\n",
      "\n",
      "Training on 16 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.09183457051961812, recall = 0.029568109326907854.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.08918345705196175, recall = 0.02807719054895115.\n",
      "\n",
      "Training on the 17 epoch\n",
      "Training on epoch 17 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693253, and regularization loss is 0.000105.\n",
      " Top K precision = 0.1010526315789473, recall = 0.025972337001598556.\n",
      "Training on epoch 17 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693289, and regularization loss is 0.000142.\n",
      " Top K precision = 0.10652173913043475, recall = 0.019657388500053642.\n",
      "Training on epoch 17 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693331, and regularization loss is 0.000184.\n",
      " Top K precision = 0.10210526315789469, recall = 0.020911703194991797.\n",
      "Training on epoch 17 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693314, and regularization loss is 0.000167.\n",
      " Top K precision = 0.08829787234042547, recall = 0.02599837201932926.\n",
      "Training on epoch 17 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693313, and regularization loss is 0.000166.\n",
      " Top K precision = 0.09574468085106379, recall = 0.023048525528981192.\n",
      "Training on epoch 17 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693297, and regularization loss is 0.00015.\n",
      " Top K precision = 0.10967741935483868, recall = 0.023958508986606415.\n",
      "Training on epoch 17 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693293, and regularization loss is 0.000146.\n",
      " Top K precision = 0.11999999999999991, recall = 0.030581464973194153.\n",
      "Training on epoch 17 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693286, and regularization loss is 0.000139.\n",
      " Top K precision = 0.10210526315789469, recall = 0.028019337060010033.\n",
      "Training on epoch 17 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693264, and regularization loss is 0.000117.\n",
      " Top K precision = 0.1141304347826086, recall = 0.027808853806423655.\n",
      "Training on epoch 17 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.693277, and regularization loss is 0.00013.\n",
      " Top K precision = 0.11170212765957441, recall = 0.023984454614969487.\n",
      "Training on epoch 17 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693279, and regularization loss is 0.000132.\n",
      " Top K precision = 0.08969072164948451, recall = 0.024086599282269062.\n",
      "Training on epoch 17 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.69326, and regularization loss is 0.000112.\n",
      " Top K precision = 0.1354166666666666, recall = 0.028606494189835036.\n",
      "Training on epoch 17 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693309, and regularization loss is 0.000162.\n",
      " Top K precision = 0.11489361702127657, recall = 0.022003101993656696.\n",
      "Training on epoch 17 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693296, and regularization loss is 0.000149.\n",
      " Top K precision = 0.11979166666666662, recall = 0.02488173508535328.\n",
      "Training on epoch 17 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693287, and regularization loss is 0.00014.\n",
      " Top K precision = 0.1030927835051546, recall = 0.02623966724634078.\n",
      "Training on epoch 17 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693272, and regularization loss is 0.000125.\n",
      " Top K precision = 0.10957446808510635, recall = 0.022630776894958703.\n",
      "Training on epoch 17 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693304, and regularization loss is 0.000157.\n",
      " Top K precision = 0.12526315789473683, recall = 0.026393979079248828.\n",
      "Training on epoch 17 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693295, and regularization loss is 0.000147.\n",
      " Top K precision = 0.11134020618556692, recall = 0.023418096131059614.\n",
      "Training on epoch 17 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693264, and regularization loss is 0.000117.\n",
      " Top K precision = 0.09285714285714283, recall = 0.020944899266064854.\n",
      "\n",
      "Training on 17 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.09278897136797441, recall = 0.029361799944953304.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.09077412513255555, recall = 0.02914535085250565.\n",
      "\n",
      "Training on the 18 epoch\n",
      "Training on epoch 18 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693263, and regularization loss is 0.000115.\n",
      " Top K precision = 0.10515463917525769, recall = 0.02602379082578083.\n",
      "Training on epoch 18 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693287, and regularization loss is 0.00014.\n",
      " Top K precision = 0.1084210526315789, recall = 0.029842080706361393.\n",
      "Training on epoch 18 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693289, and regularization loss is 0.000142.\n",
      " Top K precision = 0.09387755102040812, recall = 0.02049046184186375.\n",
      "Training on epoch 18 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693293, and regularization loss is 0.000146.\n",
      " Top K precision = 0.11938775510204078, recall = 0.025562390726361326.\n",
      "Training on epoch 18 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693319, and regularization loss is 0.000172.\n",
      " Top K precision = 0.11263157894736836, recall = 0.02661777564767876.\n",
      "Training on epoch 18 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693286, and regularization loss is 0.000139.\n",
      " Top K precision = 0.11999999999999993, recall = 0.02750790869068937.\n",
      "Training on epoch 18 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693281, and regularization loss is 0.000134.\n",
      " Top K precision = 0.11382978723404245, recall = 0.029689594505023197.\n",
      "Training on epoch 18 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693388, and regularization loss is 0.000241.\n",
      " Top K precision = 0.08453608247422675, recall = 0.020415361307575968.\n",
      "Training on epoch 18 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693277, and regularization loss is 0.00013.\n",
      " Top K precision = 0.11827956989247308, recall = 0.03027259519113403.\n",
      "Training on epoch 18 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.693264, and regularization loss is 0.000117.\n",
      " Top K precision = 0.09793814432989689, recall = 0.02577919615854049.\n",
      "Training on epoch 18 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693306, and regularization loss is 0.000159.\n",
      " Top K precision = 0.11134020618556695, recall = 0.02539979639275719.\n",
      "Training on epoch 18 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693287, and regularization loss is 0.000139.\n",
      " Top K precision = 0.10631578947368416, recall = 0.025611508361344522.\n",
      "Training on epoch 18 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693278, and regularization loss is 0.000131.\n",
      " Top K precision = 0.11382978723404247, recall = 0.02570198700263732.\n",
      "Training on epoch 18 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693331, and regularization loss is 0.000184.\n",
      " Top K precision = 0.11685393258426958, recall = 0.024007253555345113.\n",
      "Training on epoch 18 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.69331, and regularization loss is 0.000163.\n",
      " Top K precision = 0.12022471910112355, recall = 0.02425991484716122.\n",
      "Training on epoch 18 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693287, and regularization loss is 0.00014.\n",
      " Top K precision = 0.08556701030927828, recall = 0.025172977433846992.\n",
      "Training on epoch 18 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693307, and regularization loss is 0.00016.\n",
      " Top K precision = 0.10210526315789469, recall = 0.025080202599474237.\n",
      "Training on epoch 18 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693298, and regularization loss is 0.00015.\n",
      " Top K precision = 0.09795918367346931, recall = 0.023944429255215335.\n",
      "Training on epoch 18 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693254, and regularization loss is 0.000107.\n",
      " Top K precision = 0.11276595744680844, recall = 0.028926647025961486.\n",
      "\n",
      "Training on 18 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.09225874867444309, recall = 0.029614467595288442.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.09278897136797441, recall = 0.029743382062378563.\n",
      "\n",
      "Training on the 19 epoch\n",
      "Training on epoch 19 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.69333, and regularization loss is 0.000182.\n",
      " Top K precision = 0.11978021978021969, recall = 0.0294571988803221.\n",
      "Training on epoch 19 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693279, and regularization loss is 0.000132.\n",
      " Top K precision = 0.10319148936170204, recall = 0.026837406916150528.\n",
      "Training on epoch 19 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.69331, and regularization loss is 0.000163.\n",
      " Top K precision = 0.1042105263157894, recall = 0.02756376904382274.\n",
      "Training on epoch 19 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693272, and regularization loss is 0.000124.\n",
      " Top K precision = 0.08437499999999996, recall = 0.026608262548724084.\n",
      "Training on epoch 19 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.69331, and regularization loss is 0.000163.\n",
      " Top K precision = 0.11720430107526876, recall = 0.02286080005858107.\n",
      "Training on epoch 19 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693283, and regularization loss is 0.000136.\n",
      " Top K precision = 0.11758241758241751, recall = 0.025458775252902417.\n",
      "Training on epoch 19 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693317, and regularization loss is 0.00017.\n",
      " Top K precision = 0.09578947368421048, recall = 0.021286027983782974.\n",
      "Training on epoch 19 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693283, and regularization loss is 0.000135.\n",
      " Top K precision = 0.08791208791208789, recall = 0.02256166431390695.\n",
      "Training on epoch 19 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693298, and regularization loss is 0.000151.\n",
      " Top K precision = 0.10744680851063823, recall = 0.021440235857036746.\n",
      "Training on epoch 19 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.693284, and regularization loss is 0.000136.\n",
      " Top K precision = 0.09673913043478255, recall = 0.026830210029859993.\n",
      "Training on epoch 19 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693288, and regularization loss is 0.000141.\n",
      " Top K precision = 0.10404040404040397, recall = 0.023887893699435808.\n",
      "Training on epoch 19 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.69326, and regularization loss is 0.000113.\n",
      " Top K precision = 0.09263157894736837, recall = 0.021046084174941894.\n",
      "Training on epoch 19 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693314, and regularization loss is 0.000167.\n",
      " Top K precision = 0.11010101010100999, recall = 0.025091927497990923.\n",
      "Training on epoch 19 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693259, and regularization loss is 0.000112.\n",
      " Top K precision = 0.12637362637362629, recall = 0.02656242795690803.\n",
      "Training on epoch 19 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693281, and regularization loss is 0.000134.\n",
      " Top K precision = 0.11562499999999991, recall = 0.0323429340629055.\n",
      "Training on epoch 19 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693313, and regularization loss is 0.000166.\n",
      " Top K precision = 0.09574468085106377, recall = 0.024397289051474442.\n",
      "Training on epoch 19 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693297, and regularization loss is 0.00015.\n",
      " Top K precision = 0.08367346938775509, recall = 0.02713983532436323.\n",
      "Training on epoch 19 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693308, and regularization loss is 0.000161.\n",
      " Top K precision = 0.12526315789473677, recall = 0.02939488489432816.\n",
      "Training on epoch 19 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693252, and regularization loss is 0.000105.\n",
      " Top K precision = 0.09479166666666661, recall = 0.022013299934015813.\n",
      "\n",
      "Training on 19 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.09109225874867426, recall = 0.029256255015136814.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.09183457051961812, recall = 0.029412562748262802.\n",
      "\n",
      "Training on the 20 epoch\n",
      "Training on epoch 20 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693269, and regularization loss is 0.000122.\n",
      " Top K precision = 0.10434782608695647, recall = 0.02259997694843014.\n",
      "Training on epoch 20 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693309, and regularization loss is 0.000162.\n",
      " Top K precision = 0.1145833333333333, recall = 0.02541641897298071.\n",
      "Training on epoch 20 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.69328, and regularization loss is 0.000133.\n",
      " Top K precision = 0.1133333333333333, recall = 0.0326929546625051.\n",
      "Training on epoch 20 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693266, and regularization loss is 0.000119.\n",
      " Top K precision = 0.1288888888888888, recall = 0.02665081789290834.\n",
      "Training on epoch 20 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.69326, and regularization loss is 0.000113.\n",
      " Top K precision = 0.12553191489361698, recall = 0.02484554148147894.\n",
      "Training on epoch 20 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693273, and regularization loss is 0.000126.\n",
      " Top K precision = 0.11249999999999995, recall = 0.027580085302026342.\n",
      "Training on epoch 20 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693276, and regularization loss is 0.000129.\n",
      " Top K precision = 0.09368421052631573, recall = 0.025729035007977425.\n",
      "Training on epoch 20 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693374, and regularization loss is 0.000226.\n",
      " Top K precision = 0.09680851063829787, recall = 0.03087613940382641.\n",
      "Training on epoch 20 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693316, and regularization loss is 0.000168.\n",
      " Top K precision = 0.10769230769230763, recall = 0.02905769545732753.\n",
      "Training on epoch 20 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.693267, and regularization loss is 0.00012.\n",
      " Top K precision = 0.12020202020202013, recall = 0.022814446409473114.\n",
      "Training on epoch 20 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.69327, and regularization loss is 0.000122.\n",
      " Top K precision = 0.1020833333333333, recall = 0.02632182316469457.\n",
      "Training on epoch 20 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693288, and regularization loss is 0.000141.\n",
      " Top K precision = 0.12346938775510197, recall = 0.026858506540609107.\n",
      "Training on epoch 20 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.69329, and regularization loss is 0.000143.\n",
      " Top K precision = 0.11521739130434776, recall = 0.023212542910662978.\n",
      "Training on epoch 20 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693312, and regularization loss is 0.000165.\n",
      " Top K precision = 0.09157894736842101, recall = 0.024898175244797875.\n",
      "Training on epoch 20 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693265, and regularization loss is 0.000118.\n",
      " Top K precision = 0.08586956521739127, recall = 0.021334128863441378.\n",
      "Training on epoch 20 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693354, and regularization loss is 0.000207.\n",
      " Top K precision = 0.09479166666666662, recall = 0.022202823596712815.\n",
      "Training on epoch 20 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693295, and regularization loss is 0.000147.\n",
      " Top K precision = 0.10612244897959175, recall = 0.026536482729009443.\n",
      "Training on epoch 20 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693269, and regularization loss is 0.000122.\n",
      " Top K precision = 0.11134020618556695, recall = 0.024387449134431617.\n",
      "Training on epoch 20 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693304, and regularization loss is 0.000157.\n",
      " Top K precision = 0.13999999999999996, recall = 0.027280157339802647.\n",
      "\n",
      "Training on 20 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.08875927889713675, recall = 0.028123552284495867.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.08992576882290551, recall = 0.027582216038150014.\n",
      "\n",
      "Training on the 21 epoch\n",
      "Training on epoch 21 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693301, and regularization loss is 0.000154.\n",
      " Top K precision = 0.10107526881720424, recall = 0.02786676055222681.\n",
      "Training on epoch 21 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693288, and regularization loss is 0.00014.\n",
      " Top K precision = 0.09999999999999998, recall = 0.020848264405719276.\n",
      "Training on epoch 21 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693308, and regularization loss is 0.000161.\n",
      " Top K precision = 0.10618556701030918, recall = 0.022041995096974094.\n",
      "Training on epoch 21 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693283, and regularization loss is 0.000136.\n",
      " Top K precision = 0.11684210526315786, recall = 0.02422447835596132.\n",
      "Training on epoch 21 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693287, and regularization loss is 0.00014.\n",
      " Top K precision = 0.11874999999999995, recall = 0.03305473514222095.\n",
      "Training on epoch 21 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693303, and regularization loss is 0.000156.\n",
      " Top K precision = 0.10947368421052626, recall = 0.02714031373763543.\n",
      "Training on epoch 21 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693291, and regularization loss is 0.000143.\n",
      " Top K precision = 0.08888888888888886, recall = 0.0232724827200746.\n",
      "Training on epoch 21 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693272, and regularization loss is 0.000125.\n",
      " Top K precision = 0.11157894736842097, recall = 0.030323893219178617.\n",
      "Training on epoch 21 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693324, and regularization loss is 0.000177.\n",
      " Top K precision = 0.11739130434782599, recall = 0.022686794915497473.\n",
      "Training on epoch 21 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.693283, and regularization loss is 0.000136.\n",
      " Top K precision = 0.08541666666666663, recall = 0.01886416320328745.\n",
      "Training on epoch 21 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693277, and regularization loss is 0.00013.\n",
      " Top K precision = 0.10937499999999996, recall = 0.02290162846279818.\n",
      "Training on epoch 21 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.69332, and regularization loss is 0.000173.\n",
      " Top K precision = 0.1275510204081632, recall = 0.03269294648081971.\n",
      "Training on epoch 21 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693287, and regularization loss is 0.000139.\n",
      " Top K precision = 0.11734693877551007, recall = 0.030334927172282613.\n",
      "Training on epoch 21 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693308, and regularization loss is 0.000161.\n",
      " Top K precision = 0.09895833333333326, recall = 0.026890441692994562.\n",
      "Training on epoch 21 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693293, and regularization loss is 0.000145.\n",
      " Top K precision = 0.11276595744680849, recall = 0.020716074564386535.\n",
      "Training on epoch 21 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693297, and regularization loss is 0.000149.\n",
      " Top K precision = 0.1170212765957446, recall = 0.030225341946729777.\n",
      "Training on epoch 21 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693294, and regularization loss is 0.000147.\n",
      " Top K precision = 0.08043478260869562, recall = 0.021798341639250887.\n",
      "Training on epoch 21 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.69329, and regularization loss is 0.000143.\n",
      " Top K precision = 0.13092783505154634, recall = 0.026403874338148593.\n",
      "Training on epoch 21 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693264, and regularization loss is 0.000116.\n",
      " Top K precision = 0.09787234042553185, recall = 0.028342802660552346.\n",
      "\n",
      "Training on 21 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.08875927889713664, recall = 0.028580143130464683.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.09024390243902426, recall = 0.029180142430669288.\n",
      "\n",
      "Training on the 22 epoch\n",
      "Training on epoch 22 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693303, and regularization loss is 0.000156.\n",
      " Top K precision = 0.12315789473684202, recall = 0.030400375692592595.\n",
      "Training on epoch 22 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693276, and regularization loss is 0.000129.\n",
      " Top K precision = 0.10208333333333326, recall = 0.029436877013478222.\n",
      "Training on epoch 22 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693287, and regularization loss is 0.00014.\n",
      " Top K precision = 0.09263157894736837, recall = 0.020599857752097227.\n",
      "Training on epoch 22 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693254, and regularization loss is 0.000107.\n",
      " Top K precision = 0.07525773195876283, recall = 0.02290627984630904.\n",
      "Training on epoch 22 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693289, and regularization loss is 0.000141.\n",
      " Top K precision = 0.10105263157894732, recall = 0.022007915892278334.\n",
      "Training on epoch 22 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693322, and regularization loss is 0.000175.\n",
      " Top K precision = 0.1215053763440859, recall = 0.0283025219148521.\n",
      "Training on epoch 22 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693337, and regularization loss is 0.00019.\n",
      " Top K precision = 0.10624999999999986, recall = 0.027535967909808255.\n",
      "Training on epoch 22 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693283, and regularization loss is 0.000136.\n",
      " Top K precision = 0.11397849462365585, recall = 0.022815859071424374.\n",
      "Training on epoch 22 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693279, and regularization loss is 0.000131.\n",
      " Top K precision = 0.11368421052631571, recall = 0.03218895906977446.\n",
      "Training on epoch 22 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.693314, and regularization loss is 0.000167.\n",
      " Top K precision = 0.09255319148936164, recall = 0.021289617739777153.\n",
      "Training on epoch 22 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693305, and regularization loss is 0.000158.\n",
      " Top K precision = 0.10760869565217386, recall = 0.024092831460623167.\n",
      "Training on epoch 22 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693283, and regularization loss is 0.000136.\n",
      " Top K precision = 0.09247311827956987, recall = 0.023282434930905947.\n",
      "Training on epoch 22 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.6933, and regularization loss is 0.000153.\n",
      " Top K precision = 0.11632653061224481, recall = 0.033990307357634204.\n",
      "Training on epoch 22 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693333, and regularization loss is 0.000186.\n",
      " Top K precision = 0.10631578947368416, recall = 0.026527195880346667.\n",
      "Training on epoch 22 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693278, and regularization loss is 0.000131.\n",
      " Top K precision = 0.09052631578947365, recall = 0.01974764054129045.\n",
      "Training on epoch 22 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693268, and regularization loss is 0.000121.\n",
      " Top K precision = 0.1065217391304347, recall = 0.028270032441837286.\n",
      "Training on epoch 22 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693274, and regularization loss is 0.000127.\n",
      " Top K precision = 0.14408602150537628, recall = 0.027629770436435944.\n",
      "Training on epoch 22 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693296, and regularization loss is 0.000148.\n",
      " Top K precision = 0.10978260869565215, recall = 0.02124339922690379.\n",
      "Training on epoch 22 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693285, and regularization loss is 0.000137.\n",
      " Top K precision = 0.10412371134020612, recall = 0.02898128005982026.\n",
      "\n",
      "Training on 22 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.09236479321314943, recall = 0.029492502557445656.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.09098621420996805, recall = 0.028569395029040937.\n",
      "\n",
      "Training on the 23 epoch\n",
      "Training on epoch 23 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.6933, and regularization loss is 0.000152.\n",
      " Top K precision = 0.1138297872340425, recall = 0.02986776612480905.\n",
      "Training on epoch 23 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693333, and regularization loss is 0.000186.\n",
      " Top K precision = 0.09999999999999991, recall = 0.029104278552607266.\n",
      "Training on epoch 23 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693283, and regularization loss is 0.000136.\n",
      " Top K precision = 0.10526315789473678, recall = 0.021382965428344846.\n",
      "Training on epoch 23 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693317, and regularization loss is 0.00017.\n",
      " Top K precision = 0.10618556701030919, recall = 0.027729059249563846.\n",
      "Training on epoch 23 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693339, and regularization loss is 0.000192.\n",
      " Top K precision = 0.10721649484536075, recall = 0.026476020048320916.\n",
      "Training on epoch 23 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.69335, and regularization loss is 0.000203.\n",
      " Top K precision = 0.11111111111111104, recall = 0.023465484319290693.\n",
      "Training on epoch 23 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693298, and regularization loss is 0.000151.\n",
      " Top K precision = 0.10322580645161285, recall = 0.026756452784749436.\n",
      "Training on epoch 23 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693356, and regularization loss is 0.000209.\n",
      " Top K precision = 0.12765957446808504, recall = 0.022356291321676772.\n",
      "Training on epoch 23 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693274, and regularization loss is 0.000127.\n",
      " Top K precision = 0.09032258064516126, recall = 0.02086297605265057.\n",
      "Training on epoch 23 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.693322, and regularization loss is 0.000175.\n",
      " Top K precision = 0.12258064516129033, recall = 0.02460415897892966.\n",
      "Training on epoch 23 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.6933, and regularization loss is 0.000153.\n",
      " Top K precision = 0.11874999999999993, recall = 0.023999092484447065.\n",
      "Training on epoch 23 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.69334, and regularization loss is 0.000193.\n",
      " Top K precision = 0.10927835051546386, recall = 0.03177902489049095.\n",
      "Training on epoch 23 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693271, and regularization loss is 0.000124.\n",
      " Top K precision = 0.13298969072164943, recall = 0.035117298807705734.\n",
      "Training on epoch 23 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.6933, and regularization loss is 0.000153.\n",
      " Top K precision = 0.11827956989247308, recall = 0.024350764839825113.\n",
      "Training on epoch 23 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.69331, and regularization loss is 0.000162.\n",
      " Top K precision = 0.11578947368421043, recall = 0.023437402092071003.\n",
      "Training on epoch 23 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693291, and regularization loss is 0.000143.\n",
      " Top K precision = 0.10947368421052628, recall = 0.024023297057662648.\n",
      "Training on epoch 23 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693282, and regularization loss is 0.000134.\n",
      " Top K precision = 0.09999999999999999, recall = 0.02665828168195069.\n",
      "Training on epoch 23 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.69328, and regularization loss is 0.000133.\n",
      " Top K precision = 0.09999999999999996, recall = 0.03141359848760019.\n",
      "Training on epoch 23 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693231, and regularization loss is 8.4e-05.\n",
      " Top K precision = 0.11354166666666665, recall = 0.023273144061278214.\n",
      "\n",
      "Training on 23 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.09353128313891824, recall = 0.029527386940670444.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.09151643690349934, recall = 0.028168003491624887.\n",
      "\n",
      "Training on the 24 epoch\n",
      "Training on epoch 24 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.69327, and regularization loss is 0.000123.\n",
      " Top K precision = 0.09896907216494841, recall = 0.01800654657017954.\n",
      "Training on epoch 24 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693289, and regularization loss is 0.000142.\n",
      " Top K precision = 0.09999999999999994, recall = 0.022209039662586888.\n",
      "Training on epoch 24 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693307, and regularization loss is 0.000159.\n",
      " Top K precision = 0.10107526881720427, recall = 0.023445124276864817.\n",
      "Training on epoch 24 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693322, and regularization loss is 0.000175.\n",
      " Top K precision = 0.09893617021276593, recall = 0.028265872604252496.\n",
      "Training on epoch 24 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693281, and regularization loss is 0.000134.\n",
      " Top K precision = 0.10430107526881718, recall = 0.027142843881549257.\n",
      "Training on epoch 24 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693266, and regularization loss is 0.000119.\n",
      " Top K precision = 0.09791666666666655, recall = 0.02044002680296232.\n",
      "Training on epoch 24 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693296, and regularization loss is 0.000149.\n",
      " Top K precision = 0.1153061224489795, recall = 0.025043390218324302.\n",
      "Training on epoch 24 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.69331, and regularization loss is 0.000163.\n",
      " Top K precision = 0.10918367346938769, recall = 0.03103985339659987.\n",
      "Training on epoch 24 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693279, and regularization loss is 0.000132.\n",
      " Top K precision = 0.131578947368421, recall = 0.028462874199065975.\n",
      "Training on epoch 24 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.693324, and regularization loss is 0.000177.\n",
      " Top K precision = 0.10322580645161288, recall = 0.02748263553815008.\n",
      "Training on epoch 24 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693314, and regularization loss is 0.000167.\n",
      " Top K precision = 0.09591836734693872, recall = 0.022373258789380564.\n",
      "Training on epoch 24 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693319, and regularization loss is 0.000172.\n",
      " Top K precision = 0.12608695652173904, recall = 0.027761234108354352.\n",
      "Training on epoch 24 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693306, and regularization loss is 0.000158.\n",
      " Top K precision = 0.08659793814432984, recall = 0.027773598052929883.\n",
      "Training on epoch 24 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693308, and regularization loss is 0.000161.\n",
      " Top K precision = 0.09062499999999996, recall = 0.024407489914695043.\n",
      "Training on epoch 24 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693286, and regularization loss is 0.000139.\n",
      " Top K precision = 0.11649484536082468, recall = 0.027668113104563637.\n",
      "Training on epoch 24 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693267, and regularization loss is 0.00012.\n",
      " Top K precision = 0.09787234042553188, recall = 0.028221769475555523.\n",
      "Training on epoch 24 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693328, and regularization loss is 0.000181.\n",
      " Top K precision = 0.1053191489361702, recall = 0.029164540730164022.\n",
      "Training on epoch 24 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693246, and regularization loss is 9.9e-05.\n",
      " Top K precision = 0.1063829787234042, recall = 0.025751642841430063.\n",
      "Training on epoch 24 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693309, and regularization loss is 0.000162.\n",
      " Top K precision = 0.09791666666666661, recall = 0.021043718885335336.\n",
      "\n",
      "Training on 24 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.09130434782608683, recall = 0.029234575550407305.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.09289501590668067, recall = 0.02936024849585897.\n",
      "\n",
      "Training on the 25 epoch\n",
      "Training on epoch 25 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693324, and regularization loss is 0.000177.\n",
      " Top K precision = 0.12020202020202019, recall = 0.02041085555892169.\n",
      "Training on epoch 25 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693303, and regularization loss is 0.000156.\n",
      " Top K precision = 0.10526315789473682, recall = 0.021913941313161173.\n",
      "Training on epoch 25 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693284, and regularization loss is 0.000136.\n",
      " Top K precision = 0.1031914893617021, recall = 0.021974384867941073.\n",
      "Training on epoch 25 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693318, and regularization loss is 0.000171.\n",
      " Top K precision = 0.11208791208791206, recall = 0.026738742291926066.\n",
      "Training on epoch 25 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693338, and regularization loss is 0.00019.\n",
      " Top K precision = 0.10638297872340421, recall = 0.02707975613119168.\n",
      "Training on epoch 25 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693245, and regularization loss is 9.8e-05.\n",
      " Top K precision = 0.11099999999999993, recall = 0.026766235090753335.\n",
      "Training on epoch 25 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693305, and regularization loss is 0.000158.\n",
      " Top K precision = 0.09081632653061218, recall = 0.029032971002612054.\n",
      "Training on epoch 25 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693291, and regularization loss is 0.000144.\n",
      " Top K precision = 0.12608695652173904, recall = 0.02916142892511496.\n",
      "Training on epoch 25 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693306, and regularization loss is 0.000159.\n",
      " Top K precision = 0.10416666666666662, recall = 0.025776418746282337.\n",
      "Training on epoch 25 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.693284, and regularization loss is 0.000137.\n",
      " Top K precision = 0.09891304347826078, recall = 0.02649289671088536.\n",
      "Training on epoch 25 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693282, and regularization loss is 0.000135.\n",
      " Top K precision = 0.11578947368421053, recall = 0.031373633111767836.\n",
      "Training on epoch 25 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693327, and regularization loss is 0.00018.\n",
      " Top K precision = 0.12424242424242418, recall = 0.026173707074731666.\n",
      "Training on epoch 25 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693274, and regularization loss is 0.000127.\n",
      " Top K precision = 0.09473684210526312, recall = 0.029656764363616608.\n",
      "Training on epoch 25 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693307, and regularization loss is 0.000159.\n",
      " Top K precision = 0.1184782608695651, recall = 0.03151609724772772.\n",
      "Training on epoch 25 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693279, and regularization loss is 0.000132.\n",
      " Top K precision = 0.10326086956521731, recall = 0.025747476407703276.\n",
      "Training on epoch 25 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693274, and regularization loss is 0.000127.\n",
      " Top K precision = 0.0989361702127659, recall = 0.017384085873695453.\n",
      "Training on epoch 25 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693271, and regularization loss is 0.000124.\n",
      " Top K precision = 0.0923076923076923, recall = 0.020944473306415426.\n",
      "Training on epoch 25 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.69329, and regularization loss is 0.000143.\n",
      " Top K precision = 0.07979797979797977, recall = 0.018438980455489952.\n",
      "Training on epoch 25 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693272, and regularization loss is 0.000125.\n",
      " Top K precision = 0.1218749999999999, recall = 0.032043132970091946.\n",
      "\n",
      "Training on 25 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.09194061505832433, recall = 0.02960757254082752.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.09225874867444313, recall = 0.029404774051336926.\n",
      "\n",
      "Training on the 26 epoch\n",
      "Training on epoch 26 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693292, and regularization loss is 0.000145.\n",
      " Top K precision = 0.11808510638297869, recall = 0.01747941636214943.\n",
      "Training on epoch 26 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693306, and regularization loss is 0.000158.\n",
      " Top K precision = 0.12173913043478254, recall = 0.02684255276179074.\n",
      "Training on epoch 26 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693306, and regularization loss is 0.000159.\n",
      " Top K precision = 0.11063829787234035, recall = 0.025257809829313402.\n",
      "Training on epoch 26 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693284, and regularization loss is 0.000137.\n",
      " Top K precision = 0.10531914893617014, recall = 0.026162023891486207.\n",
      "Training on epoch 26 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693315, and regularization loss is 0.000168.\n",
      " Top K precision = 0.07752808988764043, recall = 0.01851515417734948.\n",
      "Training on epoch 26 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693247, and regularization loss is 0.0001.\n",
      " Top K precision = 0.10109890109890107, recall = 0.02279058345963534.\n",
      "Training on epoch 26 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693322, and regularization loss is 0.000175.\n",
      " Top K precision = 0.10520833333333324, recall = 0.02018434972647379.\n",
      "Training on epoch 26 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693324, and regularization loss is 0.000176.\n",
      " Top K precision = 0.10416666666666662, recall = 0.03163526177188978.\n",
      "Training on epoch 26 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693281, and regularization loss is 0.000134.\n",
      " Top K precision = 0.10105263157894732, recall = 0.02011268717688342.\n",
      "Training on epoch 26 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.693319, and regularization loss is 0.000171.\n",
      " Top K precision = 0.08437499999999996, recall = 0.021626323426942637.\n",
      "Training on epoch 26 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693311, and regularization loss is 0.000163.\n",
      " Top K precision = 0.09895833333333326, recall = 0.029235998157484445.\n",
      "Training on epoch 26 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693285, and regularization loss is 0.000138.\n",
      " Top K precision = 0.11632653061224485, recall = 0.02759984215097561.\n",
      "Training on epoch 26 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693291, and regularization loss is 0.000144.\n",
      " Top K precision = 0.10638297872340421, recall = 0.02311502214173546.\n",
      "Training on epoch 26 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693298, and regularization loss is 0.00015.\n",
      " Top K precision = 0.10824742268041235, recall = 0.02808212838758944.\n",
      "Training on epoch 26 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693287, and regularization loss is 0.00014.\n",
      " Top K precision = 0.11382978723404243, recall = 0.030473679730680117.\n",
      "Training on epoch 26 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693274, and regularization loss is 0.000127.\n",
      " Top K precision = 0.1133333333333333, recall = 0.02861330297616839.\n",
      "Training on epoch 26 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693293, and regularization loss is 0.000145.\n",
      " Top K precision = 0.10219780219780217, recall = 0.019170388237019904.\n",
      "Training on epoch 26 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693312, and regularization loss is 0.000165.\n",
      " Top K precision = 0.11770833333333325, recall = 0.026690141999049902.\n",
      "Training on epoch 26 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.69328, and regularization loss is 0.000132.\n",
      " Top K precision = 0.09677419354838704, recall = 0.028868074631742752.\n",
      "\n",
      "Training on 26 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.09077412513255557, recall = 0.02908593953881295.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.09257688229056191, recall = 0.029280049455701264.\n",
      "\n",
      "Training on the 27 epoch\n",
      "Training on epoch 27 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693295, and regularization loss is 0.000148.\n",
      " Top K precision = 0.11612903225806442, recall = 0.02571766308575932.\n",
      "Training on epoch 27 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693308, and regularization loss is 0.000161.\n",
      " Top K precision = 0.12371134020618549, recall = 0.03199954460600698.\n",
      "Training on epoch 27 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693323, and regularization loss is 0.000176.\n",
      " Top K precision = 0.1076086956521739, recall = 0.023695112172221967.\n",
      "Training on epoch 27 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693347, and regularization loss is 0.000199.\n",
      " Top K precision = 0.10851063829787227, recall = 0.0206944494713995.\n",
      "Training on epoch 27 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693312, and regularization loss is 0.000165.\n",
      " Top K precision = 0.09793814432989685, recall = 0.02556083138159325.\n",
      "Training on epoch 27 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693299, and regularization loss is 0.000152.\n",
      " Top K precision = 0.11428571428571423, recall = 0.023737616498079567.\n",
      "Training on epoch 27 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693325, and regularization loss is 0.000178.\n",
      " Top K precision = 0.09999999999999998, recall = 0.020517325019520277.\n",
      "Training on epoch 27 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693299, and regularization loss is 0.000152.\n",
      " Top K precision = 0.09468085106382973, recall = 0.02694062344723614.\n",
      "Training on epoch 27 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693245, and regularization loss is 9.8e-05.\n",
      " Top K precision = 0.12061855670103085, recall = 0.028113007822883104.\n",
      "Training on epoch 27 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.693315, and regularization loss is 0.000168.\n",
      " Top K precision = 0.10899999999999993, recall = 0.03028752105441482.\n",
      "Training on epoch 27 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693335, and regularization loss is 0.000188.\n",
      " Top K precision = 0.10537634408602138, recall = 0.024680693173958893.\n",
      "Training on epoch 27 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693314, and regularization loss is 0.000167.\n",
      " Top K precision = 0.1031914893617021, recall = 0.02178151979798429.\n",
      "Training on epoch 27 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693295, and regularization loss is 0.000147.\n",
      " Top K precision = 0.10520833333333325, recall = 0.026168028090330045.\n",
      "Training on epoch 27 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693288, and regularization loss is 0.000141.\n",
      " Top K precision = 0.10430107526881711, recall = 0.02331127937406375.\n",
      "Training on epoch 27 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693304, and regularization loss is 0.000157.\n",
      " Top K precision = 0.10212765957446807, recall = 0.03514140567422176.\n",
      "Training on epoch 27 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693301, and regularization loss is 0.000154.\n",
      " Top K precision = 0.1084210526315789, recall = 0.025786444208309257.\n",
      "Training on epoch 27 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693327, and regularization loss is 0.00018.\n",
      " Top K precision = 0.12187499999999996, recall = 0.020328722354498726.\n",
      "Training on epoch 27 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693349, and regularization loss is 0.000202.\n",
      " Top K precision = 0.11979166666666659, recall = 0.02838245196984437.\n",
      "Training on epoch 27 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693281, and regularization loss is 0.000134.\n",
      " Top K precision = 0.12421052631578941, recall = 0.023506296887260587.\n",
      "\n",
      "Training on 27 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.0911983032873806, recall = 0.03013900428252455.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.089289501590668, recall = 0.0283971271272328.\n",
      "\n",
      "Training on the 28 epoch\n",
      "Training on epoch 28 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693306, and regularization loss is 0.000159.\n",
      " Top K precision = 0.10624999999999996, recall = 0.024488280053208123.\n",
      "Training on epoch 28 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693273, and regularization loss is 0.000125.\n",
      " Top K precision = 0.11290322580645155, recall = 0.03061947706258442.\n",
      "Training on epoch 28 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693352, and regularization loss is 0.000204.\n",
      " Top K precision = 0.08842105263157891, recall = 0.030383019379383765.\n",
      "Training on epoch 28 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693276, and regularization loss is 0.000128.\n",
      " Top K precision = 0.10851063829787229, recall = 0.02255738740936824.\n",
      "Training on epoch 28 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693347, and regularization loss is 0.0002.\n",
      " Top K precision = 0.10851063829787226, recall = 0.0247836126941944.\n",
      "Training on epoch 28 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693353, and regularization loss is 0.000206.\n",
      " Top K precision = 0.10833333333333328, recall = 0.024862241806743415.\n",
      "Training on epoch 28 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693319, and regularization loss is 0.000171.\n",
      " Top K precision = 0.12417582417582407, recall = 0.03212444038892965.\n",
      "Training on epoch 28 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693298, and regularization loss is 0.000151.\n",
      " Top K precision = 0.1365591397849462, recall = 0.027377972127118348.\n",
      "Training on epoch 28 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693366, and regularization loss is 0.000219.\n",
      " Top K precision = 0.10879120879120874, recall = 0.028128614778239133.\n",
      "Training on epoch 28 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.693291, and regularization loss is 0.000144.\n",
      " Top K precision = 0.10425531914893615, recall = 0.02627225725220039.\n",
      "Training on epoch 28 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693297, and regularization loss is 0.00015.\n",
      " Top K precision = 0.11458333333333325, recall = 0.02579261919386784.\n",
      "Training on epoch 28 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693344, and regularization loss is 0.000197.\n",
      " Top K precision = 0.0978947368421052, recall = 0.02160549986953137.\n",
      "Training on epoch 28 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693293, and regularization loss is 0.000146.\n",
      " Top K precision = 0.09368421052631579, recall = 0.01996304594896557.\n",
      "Training on epoch 28 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693318, and regularization loss is 0.000171.\n",
      " Top K precision = 0.12872340425531908, recall = 0.02858281741945648.\n",
      "Training on epoch 28 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693288, and regularization loss is 0.000141.\n",
      " Top K precision = 0.10103092783505149, recall = 0.02369799490918494.\n",
      "Training on epoch 28 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693286, and regularization loss is 0.000139.\n",
      " Top K precision = 0.12783505154639166, recall = 0.025474712883167994.\n",
      "Training on epoch 28 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693262, and regularization loss is 0.000114.\n",
      " Top K precision = 0.11595744680851057, recall = 0.025403632586215984.\n",
      "Training on epoch 28 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693299, and regularization loss is 0.000151.\n",
      " Top K precision = 0.13195876288659789, recall = 0.020458139257904386.\n",
      "Training on epoch 28 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693298, and regularization loss is 0.000151.\n",
      " Top K precision = 0.123469387755102, recall = 0.02695005294201041.\n",
      "\n",
      "Training on 28 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.08992576882290557, recall = 0.028844125208834958.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.09098621420996805, recall = 0.028852802422171753.\n",
      "\n",
      "Training on the 29 epoch\n",
      "Training on epoch 29 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693301, and regularization loss is 0.000154.\n",
      " Top K precision = 0.10105263157894727, recall = 0.026262313743322788.\n",
      "Training on epoch 29 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693284, and regularization loss is 0.000136.\n",
      " Top K precision = 0.09999999999999992, recall = 0.024074838127587896.\n",
      "Training on epoch 29 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693263, and regularization loss is 0.000116.\n",
      " Top K precision = 0.13263157894736835, recall = 0.0253231700565285.\n",
      "Training on epoch 29 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693299, and regularization loss is 0.000152.\n",
      " Top K precision = 0.12947368421052627, recall = 0.03290422972209802.\n",
      "Training on epoch 29 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693312, and regularization loss is 0.000165.\n",
      " Top K precision = 0.0702127659574468, recall = 0.021248943743687736.\n",
      "Training on epoch 29 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693255, and regularization loss is 0.000107.\n",
      " Top K precision = 0.12187499999999997, recall = 0.02871730861052145.\n",
      "Training on epoch 29 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693316, and regularization loss is 0.000169.\n",
      " Top K precision = 0.09183673469387751, recall = 0.023611051675907335.\n",
      "Training on epoch 29 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693334, and regularization loss is 0.000187.\n",
      " Top K precision = 0.10217391304347824, recall = 0.023440446680199976.\n",
      "Training on epoch 29 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693321, and regularization loss is 0.000174.\n",
      " Top K precision = 0.1159574468085106, recall = 0.030556988316384537.\n",
      "Training on epoch 29 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.693256, and regularization loss is 0.000109.\n",
      " Top K precision = 0.12164948453608239, recall = 0.02685014239887348.\n",
      "Training on epoch 29 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.69334, and regularization loss is 0.000193.\n",
      " Top K precision = 0.10212765957446798, recall = 0.027190316134543136.\n",
      "Training on epoch 29 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693293, and regularization loss is 0.000146.\n",
      " Top K precision = 0.11489361702127655, recall = 0.02058114317342115.\n",
      "Training on epoch 29 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693282, and regularization loss is 0.000135.\n",
      " Top K precision = 0.09166666666666662, recall = 0.023352025136872397.\n",
      "Training on epoch 29 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693331, and regularization loss is 0.000184.\n",
      " Top K precision = 0.1135416666666666, recall = 0.03184708507087758.\n",
      "Training on epoch 29 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693284, and regularization loss is 0.000137.\n",
      " Top K precision = 0.11182795698924732, recall = 0.024022574162942855.\n",
      "Training on epoch 29 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693292, and regularization loss is 0.000145.\n",
      " Top K precision = 0.11684210526315783, recall = 0.030371275607746474.\n",
      "Training on epoch 29 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693296, and regularization loss is 0.000149.\n",
      " Top K precision = 0.11182795698924722, recall = 0.025269070833421076.\n",
      "Training on epoch 29 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693317, and regularization loss is 0.00017.\n",
      " Top K precision = 0.09387755102040814, recall = 0.02390113185751851.\n",
      "Training on epoch 29 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693271, and regularization loss is 0.000124.\n",
      " Top K precision = 0.09793814432989684, recall = 0.022062939560596365.\n",
      "\n",
      "Training on 29 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.0903499469777305, recall = 0.029090919207514127.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.09215270413573688, recall = 0.02980179420429062.\n",
      "\n",
      "Training on the 30 epoch\n",
      "Training on epoch 30 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693278, and regularization loss is 0.000131.\n",
      " Top K precision = 0.11752577319587627, recall = 0.022305281490785058.\n",
      "Training on epoch 30 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.69335, and regularization loss is 0.000203.\n",
      " Top K precision = 0.10319148936170208, recall = 0.023347900336293904.\n",
      "Training on epoch 30 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693272, and regularization loss is 0.000125.\n",
      " Top K precision = 0.1288659793814432, recall = 0.031985865746072366.\n",
      "Training on epoch 30 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693308, and regularization loss is 0.00016.\n",
      " Top K precision = 0.12448979591836729, recall = 0.029077175347284198.\n",
      "Training on epoch 30 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693321, and regularization loss is 0.000174.\n",
      " Top K precision = 0.11914893617021272, recall = 0.022325674885369778.\n",
      "Training on epoch 30 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693285, and regularization loss is 0.000138.\n",
      " Top K precision = 0.10520833333333329, recall = 0.03718682065070436.\n",
      "Training on epoch 30 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693287, and regularization loss is 0.00014.\n",
      " Top K precision = 0.12989690721649477, recall = 0.02671913083519029.\n",
      "Training on epoch 30 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693275, and regularization loss is 0.000127.\n",
      " Top K precision = 0.09072164948453605, recall = 0.024825353514406567.\n",
      "Training on epoch 30 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693299, and regularization loss is 0.000152.\n",
      " Top K precision = 0.12391304347826079, recall = 0.024979147655679945.\n",
      "Training on epoch 30 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.693294, and regularization loss is 0.000146.\n",
      " Top K precision = 0.09784946236559137, recall = 0.0250234082644267.\n",
      "Training on epoch 30 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693286, and regularization loss is 0.000139.\n",
      " Top K precision = 0.12659574468085102, recall = 0.030784991929323584.\n",
      "Training on epoch 30 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693272, and regularization loss is 0.000124.\n",
      " Top K precision = 0.10206185567010302, recall = 0.026506440323772733.\n",
      "Training on epoch 30 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693282, and regularization loss is 0.000135.\n",
      " Top K precision = 0.12391304347826083, recall = 0.03317137957694805.\n",
      "Training on epoch 30 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.69329, and regularization loss is 0.000143.\n",
      " Top K precision = 0.12210526315789463, recall = 0.03131372972509145.\n",
      "Training on epoch 30 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693284, and regularization loss is 0.000137.\n",
      " Top K precision = 0.11935483870967736, recall = 0.02323440702168329.\n",
      "Training on epoch 30 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693319, and regularization loss is 0.000172.\n",
      " Top K precision = 0.11808510638297866, recall = 0.024885303380845183.\n",
      "Training on epoch 30 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693255, and regularization loss is 0.000108.\n",
      " Top K precision = 0.09560439560439554, recall = 0.028048071672033447.\n",
      "Training on epoch 30 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693328, and regularization loss is 0.000181.\n",
      " Top K precision = 0.10851063829787227, recall = 0.02871165293706601.\n",
      "Training on epoch 30 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693307, and regularization loss is 0.00016.\n",
      " Top K precision = 0.09574468085106376, recall = 0.029721897671154875.\n",
      "\n",
      "Training on 30 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.09162248144220565, recall = 0.028744913264246996.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.08865323435843045, recall = 0.028340469929141375.\n",
      "\n",
      "Training on the 31 epoch\n",
      "Training on epoch 31 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693277, and regularization loss is 0.00013.\n",
      " Top K precision = 0.11063829787234039, recall = 0.031352698945885514.\n",
      "Training on epoch 31 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693335, and regularization loss is 0.000188.\n",
      " Top K precision = 0.10606060606060601, recall = 0.02380552497836762.\n",
      "Training on epoch 31 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693265, and regularization loss is 0.000118.\n",
      " Top K precision = 0.10957446808510632, recall = 0.025398872867115535.\n",
      "Training on epoch 31 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693286, and regularization loss is 0.000139.\n",
      " Top K precision = 0.07684210526315789, recall = 0.020560095434183658.\n",
      "Training on epoch 31 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693289, and regularization loss is 0.000141.\n",
      " Top K precision = 0.09891304347826084, recall = 0.024765473110842654.\n",
      "Training on epoch 31 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.69331, and regularization loss is 0.000163.\n",
      " Top K precision = 0.11752577319587623, recall = 0.02059158302713616.\n",
      "Training on epoch 31 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693295, and regularization loss is 0.000148.\n",
      " Top K precision = 0.12812499999999993, recall = 0.027061977550789346.\n",
      "Training on epoch 31 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693278, and regularization loss is 0.000131.\n",
      " Top K precision = 0.09072164948453605, recall = 0.028392638715115038.\n",
      "Training on epoch 31 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.69329, and regularization loss is 0.000143.\n",
      " Top K precision = 0.08085106382978724, recall = 0.015738077956275656.\n",
      "Training on epoch 31 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.693301, and regularization loss is 0.000154.\n",
      " Top K precision = 0.10215053763440854, recall = 0.026978399610233964.\n",
      "Training on epoch 31 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693289, and regularization loss is 0.000142.\n",
      " Top K precision = 0.10108695652173912, recall = 0.02007760822847643.\n",
      "Training on epoch 31 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693277, and regularization loss is 0.00013.\n",
      " Top K precision = 0.10421052631578943, recall = 0.025106107865927912.\n",
      "Training on epoch 31 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693297, and regularization loss is 0.00015.\n",
      " Top K precision = 0.10531914893617014, recall = 0.02927109641652523.\n",
      "Training on epoch 31 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693296, and regularization loss is 0.000149.\n",
      " Top K precision = 0.10105263157894727, recall = 0.01900978310462007.\n",
      "Training on epoch 31 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693281, and regularization loss is 0.000134.\n",
      " Top K precision = 0.1061855670103092, recall = 0.02423638214228411.\n",
      "Training on epoch 31 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693284, and regularization loss is 0.000137.\n",
      " Top K precision = 0.13191489361702122, recall = 0.032941350280494254.\n",
      "Training on epoch 31 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.6933, and regularization loss is 0.000153.\n",
      " Top K precision = 0.10652173913043471, recall = 0.02391674933040548.\n",
      "Training on epoch 31 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693298, and regularization loss is 0.000151.\n",
      " Top K precision = 0.11649484536082462, recall = 0.026025452784829935.\n",
      "Training on epoch 31 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693278, and regularization loss is 0.00013.\n",
      " Top K precision = 0.10208333333333329, recall = 0.027106521580665056.\n",
      "\n",
      "Training on 31 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.08971367974549306, recall = 0.028959820287600895.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.09013785790031803, recall = 0.029571895952835235.\n",
      "\n",
      "Training on the 32 epoch\n",
      "Training on epoch 32 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693326, and regularization loss is 0.000179.\n",
      " Top K precision = 0.12315789473684206, recall = 0.030907992033129286.\n",
      "Training on epoch 32 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693307, and regularization loss is 0.00016.\n",
      " Top K precision = 0.10531914893617018, recall = 0.028570899313888693.\n",
      "Training on epoch 32 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693256, and regularization loss is 0.000109.\n",
      " Top K precision = 0.10927835051546383, recall = 0.024788824051735082.\n",
      "Training on epoch 32 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693265, and regularization loss is 0.000118.\n",
      " Top K precision = 0.12173913043478256, recall = 0.023683144915168416.\n",
      "Training on epoch 32 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693295, and regularization loss is 0.000148.\n",
      " Top K precision = 0.13999999999999996, recall = 0.023863661155131367.\n",
      "Training on epoch 32 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693278, and regularization loss is 0.000131.\n",
      " Top K precision = 0.1072916666666666, recall = 0.02837862051383248.\n",
      "Training on epoch 32 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693342, and regularization loss is 0.000195.\n",
      " Top K precision = 0.10674157303370782, recall = 0.02370421224563632.\n",
      "Training on epoch 32 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.69327, and regularization loss is 0.000123.\n",
      " Top K precision = 0.1010416666666666, recall = 0.022287154058074635.\n",
      "Training on epoch 32 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.6933, and regularization loss is 0.000153.\n",
      " Top K precision = 0.12978723404255316, recall = 0.030563510042826456.\n",
      "Training on epoch 32 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.69332, and regularization loss is 0.000173.\n",
      " Top K precision = 0.10208333333333326, recall = 0.02649776189640551.\n",
      "Training on epoch 32 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693306, and regularization loss is 0.000159.\n",
      " Top K precision = 0.11649484536082468, recall = 0.02736583496810015.\n",
      "Training on epoch 32 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693354, and regularization loss is 0.000207.\n",
      " Top K precision = 0.10752688172043005, recall = 0.031068910703001884.\n",
      "Training on epoch 32 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693331, and regularization loss is 0.000184.\n",
      " Top K precision = 0.10421052631578938, recall = 0.024578501916175454.\n",
      "Training on epoch 32 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693361, and regularization loss is 0.000214.\n",
      " Top K precision = 0.10631578947368416, recall = 0.027117685940965284.\n",
      "Training on epoch 32 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693254, and regularization loss is 0.000107.\n",
      " Top K precision = 0.10319148936170204, recall = 0.03367785719589312.\n",
      "Training on epoch 32 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693355, and regularization loss is 0.000208.\n",
      " Top K precision = 0.11263157894736836, recall = 0.026852338517149354.\n",
      "Training on epoch 32 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693269, and regularization loss is 0.000121.\n",
      " Top K precision = 0.10212765957446808, recall = 0.0250883821946758.\n",
      "Training on epoch 32 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.69336, and regularization loss is 0.000213.\n",
      " Top K precision = 0.10624999999999996, recall = 0.031145358676055557.\n",
      "Training on epoch 32 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693253, and regularization loss is 0.000105.\n",
      " Top K precision = 0.12173913043478249, recall = 0.025483023765993786.\n",
      "\n",
      "Training on 32 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.09183457051961827, recall = 0.029462607526353088.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.09130434782608685, recall = 0.029283561240140147.\n",
      "\n",
      "Training on the 33 epoch\n",
      "Training on epoch 33 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693291, and regularization loss is 0.000144.\n",
      " Top K precision = 0.11111111111111104, recall = 0.02808837087337575.\n",
      "Training on epoch 33 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693271, and regularization loss is 0.000124.\n",
      " Top K precision = 0.12111111111111106, recall = 0.030357041512906878.\n",
      "Training on epoch 33 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693321, and regularization loss is 0.000174.\n",
      " Top K precision = 0.11914893617021269, recall = 0.028994697124591705.\n",
      "Training on epoch 33 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693297, and regularization loss is 0.00015.\n",
      " Top K precision = 0.10526315789473678, recall = 0.025038136742842373.\n",
      "Training on epoch 33 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693264, and regularization loss is 0.000117.\n",
      " Top K precision = 0.09555555555555549, recall = 0.028575878064703183.\n",
      "Training on epoch 33 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693283, and regularization loss is 0.000136.\n",
      " Top K precision = 0.10631578947368416, recall = 0.03053048374250535.\n",
      "Training on epoch 33 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693289, and regularization loss is 0.000141.\n",
      " Top K precision = 0.11052631578947361, recall = 0.021356263989734055.\n",
      "Training on epoch 33 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693295, and regularization loss is 0.000148.\n",
      " Top K precision = 0.12499999999999989, recall = 0.026910147516912907.\n",
      "Training on epoch 33 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693299, and regularization loss is 0.000152.\n",
      " Top K precision = 0.11666666666666663, recall = 0.024749822903357654.\n",
      "Training on epoch 33 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.693364, and regularization loss is 0.000217.\n",
      " Top K precision = 0.10999999999999995, recall = 0.02822687675530902.\n",
      "Training on epoch 33 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693281, and regularization loss is 0.000134.\n",
      " Top K precision = 0.09999999999999996, recall = 0.020298086105621758.\n",
      "Training on epoch 33 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693251, and regularization loss is 0.000104.\n",
      " Top K precision = 0.08829787234042548, recall = 0.024931711466926847.\n",
      "Training on epoch 33 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693282, and regularization loss is 0.000135.\n",
      " Top K precision = 0.12828282828282822, recall = 0.03161066897407617.\n",
      "Training on epoch 33 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693294, and regularization loss is 0.000147.\n",
      " Top K precision = 0.11157894736842099, recall = 0.026747274854267256.\n",
      "Training on epoch 33 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.69327, and regularization loss is 0.000123.\n",
      " Top K precision = 0.11473684210526312, recall = 0.02328953185801017.\n",
      "Training on epoch 33 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693339, and regularization loss is 0.000192.\n",
      " Top K precision = 0.10744680851063823, recall = 0.024030179971456504.\n",
      "Training on epoch 33 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693353, and regularization loss is 0.000206.\n",
      " Top K precision = 0.13020833333333323, recall = 0.028250855979724685.\n",
      "Training on epoch 33 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693272, and regularization loss is 0.000124.\n",
      " Top K precision = 0.09468085106382977, recall = 0.02332409389414847.\n",
      "Training on epoch 33 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693315, and regularization loss is 0.000168.\n",
      " Top K precision = 0.10888888888888884, recall = 0.026714643341168874.\n",
      "\n",
      "Training on 33 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.09151643690349943, recall = 0.029353067283570077.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.09172852598091186, recall = 0.029355890995598512.\n",
      "\n",
      "Training on the 34 epoch\n",
      "Training on epoch 34 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693274, and regularization loss is 0.000127.\n",
      " Top K precision = 0.10752688172043003, recall = 0.026139672692050885.\n",
      "Training on epoch 34 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.69332, and regularization loss is 0.000173.\n",
      " Top K precision = 0.1104166666666666, recall = 0.026324835779082702.\n",
      "Training on epoch 34 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.693284, and regularization loss is 0.000136.\n",
      " Top K precision = 0.10645161290322572, recall = 0.023359963421784587.\n",
      "Training on epoch 34 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693295, and regularization loss is 0.000148.\n",
      " Top K precision = 0.1073684210526315, recall = 0.03614884826174232.\n",
      "Training on epoch 34 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693275, and regularization loss is 0.000128.\n",
      " Top K precision = 0.11702127659574459, recall = 0.031031018583499336.\n",
      "Training on epoch 34 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693317, and regularization loss is 0.00017.\n",
      " Top K precision = 0.10421052631578943, recall = 0.029794868068321555.\n",
      "Training on epoch 34 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693279, and regularization loss is 0.000132.\n",
      " Top K precision = 0.09999999999999995, recall = 0.024320448504477166.\n",
      "Training on epoch 34 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693326, and regularization loss is 0.000179.\n",
      " Top K precision = 0.1354166666666666, recall = 0.030418874459240908.\n",
      "Training on epoch 34 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.69331, and regularization loss is 0.000163.\n",
      " Top K precision = 0.0917525773195876, recall = 0.01954512595392302.\n",
      "Training on epoch 34 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.693275, and regularization loss is 0.000128.\n",
      " Top K precision = 0.10736842105263153, recall = 0.023385285794858775.\n",
      "Training on epoch 34 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693309, and regularization loss is 0.000162.\n",
      " Top K precision = 0.08105263157894733, recall = 0.024636397343786014.\n",
      "Training on epoch 34 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693266, and regularization loss is 0.000119.\n",
      " Top K precision = 0.11276595744680845, recall = 0.025636004176994655.\n",
      "Training on epoch 34 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.6933, and regularization loss is 0.000153.\n",
      " Top K precision = 0.1072916666666666, recall = 0.023916859510979823.\n",
      "Training on epoch 34 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693293, and regularization loss is 0.000146.\n",
      " Top K precision = 0.10430107526881711, recall = 0.027005847592623082.\n",
      "Training on epoch 34 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693259, and regularization loss is 0.000112.\n",
      " Top K precision = 0.12604166666666655, recall = 0.028601344258883114.\n",
      "Training on epoch 34 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693278, and regularization loss is 0.000131.\n",
      " Top K precision = 0.1212121212121211, recall = 0.03635910818435627.\n",
      "Training on epoch 34 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693268, and regularization loss is 0.000121.\n",
      " Top K precision = 0.11222222222222222, recall = 0.024180218710905733.\n",
      "Training on epoch 34 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693298, and regularization loss is 0.00015.\n",
      " Top K precision = 0.11578947368421043, recall = 0.03128339948936353.\n",
      "Training on epoch 34 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.693288, and regularization loss is 0.000141.\n",
      " Top K precision = 0.11313131313131304, recall = 0.02570946247137074.\n",
      "\n",
      "Training on 34 epoch completed.\n",
      " Average bpr_loss on train set is 0.006933 for the current epoch.\n",
      " Training top K precision = 0.09204665959703076, recall = 0.029777772344041425.\n",
      " Average bpr_loss on the validation set is 4e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.09162248144220558, recall = 0.028210434104018373.\n",
      "\n",
      "Training on the 35 epoch\n",
      "Training on epoch 35 minibatch 1/1886 completed\n",
      " bpr_loss on current minibatch is 0.693289, and regularization loss is 0.000142.\n",
      " Top K precision = 0.11182795698924726, recall = 0.024246265601689868.\n",
      "Training on epoch 35 minibatch 101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693285, and regularization loss is 0.000138.\n",
      " Top K precision = 0.11789473684210518, recall = 0.027154942439695035.\n",
      "Training on epoch 35 minibatch 201/1886 completed\n",
      " bpr_loss on current minibatch is 0.6933, and regularization loss is 0.000153.\n",
      " Top K precision = 0.1, recall = 0.028360071650552605.\n",
      "Training on epoch 35 minibatch 301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693255, and regularization loss is 0.000108.\n",
      " Top K precision = 0.0989361702127659, recall = 0.027398450729992148.\n",
      "Training on epoch 35 minibatch 401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693301, and regularization loss is 0.000154.\n",
      " Top K precision = 0.10105263157894735, recall = 0.021626172885319723.\n",
      "Training on epoch 35 minibatch 501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693304, and regularization loss is 0.000157.\n",
      " Top K precision = 0.10736842105263153, recall = 0.021126820815583005.\n",
      "Training on epoch 35 minibatch 601/1886 completed\n",
      " bpr_loss on current minibatch is 0.693257, and regularization loss is 0.00011.\n",
      " Top K precision = 0.12340425531914892, recall = 0.026642587253871013.\n",
      "Training on epoch 35 minibatch 701/1886 completed\n",
      " bpr_loss on current minibatch is 0.693318, and regularization loss is 0.000171.\n",
      " Top K precision = 0.13936170212765947, recall = 0.021939074974595404.\n",
      "Training on epoch 35 minibatch 801/1886 completed\n",
      " bpr_loss on current minibatch is 0.69327, and regularization loss is 0.000122.\n",
      " Top K precision = 0.09361702127659567, recall = 0.022225189657363663.\n",
      "Training on epoch 35 minibatch 901/1886 completed\n",
      " bpr_loss on current minibatch is 0.693317, and regularization loss is 0.00017.\n",
      " Top K precision = 0.12282608695652168, recall = 0.02950835793590065.\n",
      "Training on epoch 35 minibatch 1001/1886 completed\n",
      " bpr_loss on current minibatch is 0.693272, and regularization loss is 0.000125.\n",
      " Top K precision = 0.10306122448979586, recall = 0.023860656239791198.\n",
      "Training on epoch 35 minibatch 1101/1886 completed\n",
      " bpr_loss on current minibatch is 0.693295, and regularization loss is 0.000148.\n",
      " Top K precision = 0.09897959183673463, recall = 0.023720794168445695.\n",
      "Training on epoch 35 minibatch 1201/1886 completed\n",
      " bpr_loss on current minibatch is 0.69329, and regularization loss is 0.000143.\n",
      " Top K precision = 0.09484536082474224, recall = 0.017652004115949638.\n",
      "Training on epoch 35 minibatch 1301/1886 completed\n",
      " bpr_loss on current minibatch is 0.693312, and regularization loss is 0.000164.\n",
      " Top K precision = 0.1040816326530612, recall = 0.02964494563794776.\n",
      "Training on epoch 35 minibatch 1401/1886 completed\n",
      " bpr_loss on current minibatch is 0.693282, and regularization loss is 0.000135.\n",
      " Top K precision = 0.10210526315789471, recall = 0.026665080489310055.\n",
      "Training on epoch 35 minibatch 1501/1886 completed\n",
      " bpr_loss on current minibatch is 0.693283, and regularization loss is 0.000135.\n",
      " Top K precision = 0.11340206185567003, recall = 0.023516553615324697.\n",
      "Training on epoch 35 minibatch 1601/1886 completed\n",
      " bpr_loss on current minibatch is 0.69335, and regularization loss is 0.000203.\n",
      " Top K precision = 0.1305263157894736, recall = 0.02722705825673066.\n",
      "Training on epoch 35 minibatch 1701/1886 completed\n",
      " bpr_loss on current minibatch is 0.69329, and regularization loss is 0.000143.\n",
      " Top K precision = 0.09583333333333327, recall = 0.02299138243997205.\n",
      "Training on epoch 35 minibatch 1801/1886 completed\n",
      " bpr_loss on current minibatch is 0.6933, and regularization loss is 0.000152.\n",
      " Top K precision = 0.1212765957446808, recall = 0.02594329491396018.\n"
     ]
    }
   ],
   "source": [
    "epochs_tracked = []\n",
    "train_topks = []\n",
    "val_topks = []\n",
    "bprs = []\n",
    "K = config_dict[\"K\"]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Training on the {} epoch\".format(epoch))\n",
    "    lightGCN.train()\n",
    "    loss_sum = 0\n",
    "    # Shuffle the order of rows.\n",
    "    samples_train = samples_train[torch.randperm(samples_train.size()[0])]\n",
    "    for batch_idx in range(math.ceil(len(samples_train) / batch_size)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        current_batch = \\\n",
    "            samples_train[batch_idx*batch_size: (batch_idx+1)*batch_size]\n",
    "        # Shuffle the order of rows.\n",
    "        current_batch = current_batch[torch.randperm(current_batch.size()[0])]\n",
    "        users = current_batch[:, 0:1]\n",
    "        pos = current_batch[:, 1:2]\n",
    "        neg = current_batch[:, 2:3]\n",
    "\n",
    "        loss, reg_loss = bpr_loss(lightGCN, users, pos, neg, mov,\n",
    "                                  train_mask)\n",
    "        reg_loss = reg_loss * weight_decay\n",
    "        loss = loss + reg_loss\n",
    "        loss_sum += loss.detach()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % config_dict[\"minibatch_per_print\"] == 0:\n",
    "            all_users = torch.linspace(start=0,\n",
    "                                       end=n_users - 1, steps=n_users).long()\n",
    "            user_indices = current_batch[:, 0]\n",
    "            user_indices = user_indices.repeat(2).long()\n",
    "            item_indices = torch.cat(\n",
    "                (current_batch[:, 1], current_batch[:, 2])).long()\n",
    "            pred = getUsersRating(lightGCN,\n",
    "                                  all_users,\n",
    "                                  mov)[user_indices, item_indices]\n",
    "            truth = global_edge_index[user_indices, item_indices]\n",
    "            topk_precision, topk_recall = \\\n",
    "                personalized_topk(pred, K, user_indices, global_edge_index)\n",
    "\n",
    "            print(\"Training on epoch {} minibatch {}/{} completed\\n\".format(epoch, batch_idx+1,\n",
    "                                                                            math.ceil(len(samples_train) / batch_size)),\n",
    "                  \"bpr_loss on current minibatch is {}, and regularization loss is {}.\\n\".format(round(float(loss.detach().cpu()), 6),\n",
    "                                                                                                 round(float(reg_loss.detach().cpu()), 6)),\n",
    "                  \"Top K precision = {}, recall = {}.\".format(topk_precision, topk_recall))\n",
    "\n",
    "    if epoch % config_dict[\"epochs_per_print\"] == 0:\n",
    "        epochs_tracked.append(epoch)\n",
    "\n",
    "        # evaluation on both the trainisng and validation set\n",
    "        lightGCN.eval()\n",
    "        # predict on the training set\n",
    "        users = samples_train[:, 0:1]\n",
    "        user_indices = samples_train[:, 0]\n",
    "        user_indices = user_indices.repeat(2).long()\n",
    "        item_indices = torch.cat(\n",
    "            (samples_train[:, 1], samples_train[:, 2])).long()\n",
    "        pred = getUsersRating(lightGCN,\n",
    "                              users[:,0],\n",
    "                              mov)[user_indices, item_indices]\n",
    "        truth = global_edge_index[users.long()[:,0]]\\\n",
    "            [user_indices, item_indices]\n",
    "        train_topk_precision, train_topk_recall = \\\n",
    "            personalized_topk(pred, K, user_indices, global_edge_index)\n",
    "        train_topks.append((train_topk_precision, train_topk_recall))\n",
    "\n",
    "        # predict on the validation set\n",
    "        users_val = samples_val[:, 0:1]\n",
    "        pos_val = samples_val[:, 1:2]\n",
    "        neg_val = samples_val[:, 2:3]\n",
    "\n",
    "        loss_val, reg_loss_val = bpr_loss(\n",
    "            lightGCN, users_val, pos_val, neg_val, mov, val_mask)\n",
    "        bprs.append(round(float((loss_val+reg_loss_val)/len(samples_val)), 6))\n",
    "        reg_loss_val = reg_loss_val * weight_decay\n",
    "\n",
    "        # predict on the validation set\n",
    "        user_indices = samples_val[:, 0]\n",
    "        user_indices = user_indices.repeat(2).long()\n",
    "        item_indices = torch.cat((samples_val[:, 1], samples_val[:, 2])).long()\n",
    "        pred_val = getUsersRating(lightGCN,\n",
    "                                  users_val[:,0],\n",
    "                                  mov)[user_indices, item_indices]\n",
    "        truth_val = global_edge_index[users_val.long()[:,0]]\\\n",
    "            [user_indices, item_indices]\n",
    "        val_topk_precision, val_topk_recall = \\\n",
    "            personalized_topk(pred_val, K, user_indices, global_edge_index)\n",
    "        val_topks.append((val_topk_precision, val_topk_recall))\n",
    "\n",
    "        print(\"\\nTraining on {} epoch completed.\\n\".format(epoch),\n",
    "              \"Average bpr_loss on train set is {} for the current epoch.\\n\".format(round(float(loss_sum/len(samples_train)), 6)),\n",
    "              \"Training top K precision = {}, recall = {}.\\n\".format(train_topk_precision, train_topk_recall),\n",
    "              \"Average bpr_loss on the validation set is {}, and regularization loss is {}.\\n\".format(round(float((loss_val+reg_loss_val)/len(samples_val)), 6),\n",
    "                                                                                                      round(float(reg_loss_val/len(samples_val)), 6)),\n",
    "              \"Validation top K precision = {}, recall = {}.\\n\".format(val_topk_precision, val_topk_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5740ad-ce2c-44cd-853f-dac2bad6e06d",
   "metadata": {},
   "source": [
    "# Plot Top K over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f940387-40e0-4216-8597-c072f804d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(epochs_tracked, [precision for precision, _ in train_topks],\n",
    "         label=\"Train\")\n",
    "plt.plot(epochs_tracked, [precision for precision, _ in val_topks],\n",
    "         label=\"Val\")\n",
    "#plt.plot(epochs_tracked, [loss for loss in bprs],\n",
    "#         label=\"BPR Loss\")\n",
    "plt.ylabel(f\"Top {K} precision\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "plt.title(\"Top 10 Precision for LightGCN on MovieLens100K (500 samples/user)\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochs_tracked, [loss for loss in bprs],\n",
    "         label=\"BPR Loss\")\n",
    "plt.ylabel(f\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362ce4f8-dafb-4874-949e-b4f5f7bb3b33",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db294ae3-07df-4550-a387-fd4100a09696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the test set\n",
    "lightGCN.eval()\n",
    "print(\"Training completed after {} epochs\".format(epochs))\n",
    "\n",
    "users_test = samples_test[:, 0:1]\n",
    "pos_test = samples_test[:, 1:2]\n",
    "neg_test = samples_test[:, 2:3]\n",
    "\n",
    "loss_test, reg_loss_test = bpr_loss(\n",
    "    lightGCN, users_test, pos_test, neg_test, mov, test_mask)\n",
    "reg_loss_test = reg_loss_test * weight_decay\n",
    "\n",
    "# predict on the test set\n",
    "user_indices = samples_test[:, 0]\n",
    "user_indices = user_indices.repeat(2).long()\n",
    "item_indices = torch.cat((samples_test[:, 1], samples_test[:, 2])).long()\n",
    "pred_test = getUsersRating(lightGCN, users_test[:,0], mov)\\\n",
    "    [user_indices, item_indices]\n",
    "truth_test = global_edge_index[users_test.long()[:,0]]\\\n",
    "    [user_indices, item_indices]\n",
    "test_topk_precision, test_topk_recall = personalized_topk(\n",
    "    pred_test, K, user_indices, global_edge_index)\n",
    "\n",
    "print(\"Average bpr_loss on the test set is {}, and regularization loss is {}.\\n\".format(round(float((loss_test+reg_loss_test)/len(samples_test)), 6),\n",
    "                                                                                                round(float(reg_loss_test/len(samples_test)), 6)),\n",
    "      \"Top K precision = {}, recall = {}.\".format(test_topk_precision, test_topk_recall))\n",
    "\n",
    "# Save model embeddings.\n",
    "torch.save(lightGCN, config_dict[\"model_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d33dad-8bb4-4f4b-8874-b1fa1c6c1609",
   "metadata": {},
   "source": [
    "# Baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c8a704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorly import decomposition\n",
    "\n",
    "def matrix_factorization(user_item, rank):\n",
    "    \"\"\"Runs matrix factorization on `user_item` and get user-item similarities.\n",
    "\n",
    "    Args:\n",
    "        user_item: User-item connectivity matrix.\n",
    "        rank: Number of numbers to represent a user / item.\n",
    "\n",
    "    Returns:\n",
    "        User-item similarities.\n",
    "    \"\"\"\n",
    "    weights, (user_factors, item_factors) = \\\n",
    "        decomposition.parafac(user_item, rank)\n",
    "    similarities = user_factors @ item_factors.T\n",
    "    return 1 / (1 + np.exp(- similarities))\n",
    "\n",
    "# Compute baseline metrics using matrix factorization.\n",
    "user_indices = samples_test[:, 0]\n",
    "user_indices = user_indices.repeat(2).long()\n",
    "item_indices = torch.cat((samples_test[:, 1], samples_test[:, 2])).long()\n",
    "baseline_pred = matrix_factorization(\n",
    "        global_edge_index.detach().cpu().numpy(),\n",
    "        config_dict[\"mf_rank\"])[user_indices.cpu(), item_indices.cpu()]\n",
    "baseline_topk_precision, baseline_topk_recall = \\\n",
    "        personalized_topk(baseline_pred, config_dict[\"K\"], user_indices, global_edge_index)\n",
    "print(\"Baseline (PARAFAC matrix factorization) produces \",\n",
    "      \"Top K precision = {}, recall = {}.\".format(baseline_topk_precision,\n",
    "                                                  baseline_topk_recall))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
